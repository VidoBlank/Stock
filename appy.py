

import tushare as ts
from config import Config as AppConfig
import pandas as pd
import json
import os
import requests
import gradio as gr
import matplotlib.pyplot as plt
from datetime import datetime, timedelta
from pandas.tseries.holiday import get_calendar
import logging
from functools import lru_cache
import numpy as np
import concurrent.futures
from typing import List, Tuple, Dict, Optional
from typing import Optional, Tuple, Dict, Any
import time
import threading
from tqdm import tqdm
import concurrent.futures
import traceback
import sys
import appy
from collections import defaultdict
from typing import Set  # æ·»åŠ åˆ°æ–‡ä»¶é¡¶éƒ¨å¯¼å…¥éƒ¨åˆ†
sys.stdout.reconfigure(encoding='utf-8')
import io
import functools  # æ·»åŠ æ­¤è¡Œ

sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')

from fetch_a_shares_data import API_CONFIG


# å¼ºåˆ¶ stdout/stderr ä½¿ç”¨ UTF-8
sys.stdout.reconfigure(encoding='utf-8')
sys.stderr.reconfigure(encoding='utf-8')

# æ–°å¢é…ç½®é¡¹
IS_BACKTEST = False  # é»˜è®¤éå›æµ‹æ¨¡å¼
CURRENT_TRADE_DATE = None  # å›æµ‹æ—¶è®¾ç½®å½“å‰æ—¥æœŸ


# ===== å…¨å±€å®šä¹‰ =====

STRATEGY_GROUPS = {
    # ===== æ ¸å¿ƒè¶‹åŠ¿ç­–ç•¥ =====
    "è¶‹åŠ¿å‹": [
        "å‡çº¿çªç ´ï¼ˆ5/20/30æ—¥ï¼‰",  # åˆå¹¶åŸ"ç«™ä¸ŠXæ—¥å‡çº¿"ç³»åˆ—
        "å‡çº¿å¤šå¤´æ’åˆ—", 
        "MACDé›¶è½´å…±æŒ¯",          # åˆå¹¶"MACDé‡‘å‰"å’Œ"MACDé›¶è½´é‡‘å‰"
        "è¶‹åŠ¿çªç ´ç¡®è®¤"            # åˆå¹¶"çªç ´å‹åŠ›ä½"+"çªç ´5æ—¥çº¿æ”¾é‡"
    ],
    
    # ===== é‡ä»·åŠ¨èƒ½ç­–ç•¥ =====
    "åŠ¨é‡å‹": [
        "é‡ä»·é½å‡",              # åˆå¹¶"æ¸©å’Œæ”¾é‡"+"çªç„¶æ”¾é‡"
        "ä¸»åŠ›èµ„é‡‘å…±æŒ¯",          # åˆå¹¶"é¾™è™æ¦œå¼‚åŠ¨"+"ä¸»åŠ›å‡€æµå…¥"+"èèµ„å‡€ä¹°å…¥"
        "OBVåŠ¨é‡å¼•æ“"           # å‡çº§åŸ"OBVä¸Šç©¿å‡çº¿"
    ],
    
    # ===== åº•éƒ¨åè½¬ç­–ç•¥ =====
    "åè½¬å‹": [
        "è¶…è·Œåå¼¹ï¼ˆRSI+BOLLï¼‰",  # åˆå¹¶"RSIè¶…å–å›å‡"+"BOLLä¸‹è½¨åå¼¹"
        "åº•éƒ¨åè½¬ç¡®è®¤"           # æ–°å¢å¤åˆä¿¡å·
    ],
    
    # ===== å¯¹å†²ç­–ç•¥ =====
    "å¸‚åœºä¸­æ€§å‹": [
        "è¡Œä¸šè¶…é¢æ”¶ç›Šï¼ˆRSæ”¹è¿›ç‰ˆï¼‰", 
        "æ³¢åŠ¨ç‡å¥—åˆ©",
        "é‡ä»·èƒŒç¦»"
    ],
    
    # ===== é£é™©æ§åˆ¶ =====
    "é£é™©å‹": [
        "è¶‹åŠ¿ç ´ä½ï¼ˆMA60+MACDæ­»å‰ï¼‰",  # åˆå¹¶åŸä¸¤é¡¹
        "é«˜ä½æ»æ¶¨é£é™©"              # æ–°å¢
    ],
    
    # ===== æ–°å¢: ç©¿çº¿å‹ç­–ç•¥ =====
    "ç©¿çº¿å‹": [
        "ä¸€é˜³ç©¿ä¸‰çº¿",             # æ–°å¢æŠ€æœ¯æŒ‡æ ‡
    ]
}

# ===== ç­–ç•¥æƒé‡å®šä¹‰ =====
STRATEGY_WEIGHTS = {
    # === è¶‹åŠ¿å‹ ===
    "å‡çº¿çªç ´ï¼ˆ5/20/30æ—¥ï¼‰": 25,  
    "å‡çº¿å¤šå¤´æ’åˆ—": 28,             
    "MACDé›¶è½´å…±æŒ¯": 32,             
    "è¶‹åŠ¿çªç ´ç¡®è®¤": 35,            
    
    # === åŠ¨é‡å‹ ===
    "é‡ä»·é½å‡": 35,                 
    "ä¸»åŠ›èµ„é‡‘å…±æŒ¯": 38,            
    "OBVåŠ¨é‡å¼•æ“": 37,             
    
    # === åè½¬å‹ ===
    "è¶…è·Œåå¼¹ï¼ˆRSI+BOLLï¼‰": 35,   
    "åº•éƒ¨åè½¬ç¡®è®¤": 40,
    
    # === é£é™©å‹ ===
    "è¶‹åŠ¿ç ´ä½ï¼ˆMA60+MACDæ­»å‰ï¼‰": 25,  
    "é«˜ä½æ»æ¶¨é£é™©": 20,
    
    # === ç©¿çº¿å‹ ===
    "ä¸€é˜³ç©¿ä¸‰çº¿": 45,              
}


# ===== æ¿å—/å¸‚åœºæ˜ å°„ =====
MARKET_SECTORS = {
    "ä¸»æ¿": ["ä¸»æ¿"],
    "åˆ›ä¸šæ¿": ["åˆ›ä¸šæ¿"],
    "ç§‘åˆ›æ¿": ["ç§‘åˆ›æ¿"],
    "ä¸­è¯ç™½é…’": [],
    "ä¸­è¯æ¶ˆè´¹": [],
    "å›½è¯ETF": [],
    "ä¸­è¯500": [],
    "æ·±è¯100": [],
    "åŒ—è¯50": [],
    "ç§‘åˆ›50": [],
    "æ²ªæ·±300": [],
    "ä¸Šè¯50": []
}

STRATEGY_TYPE_WEIGHTS = {
    "è¶‹åŠ¿å‹": 1.0,  
    "åŠ¨é‡å‹": 1.0,
    "åè½¬å‹": 1.0,   
    "é£é™©å‹": -1.0,  
    "å¸‚åœºä¸­æ€§å‹": 1.0,
    "ç©¿çº¿å‹": 1.0
}




# åŠ¨æ€æ‰£åˆ†ç³»æ•°é…ç½®
RISK_PENALTY_MULTIPLIER = {
    "è¶‹åŠ¿ç ´ä½ï¼ˆMA60+MACDæ­»å‰ï¼‰": 1.0,  # é‡å¤§é£é™©åŠ å€æ‰£åˆ†
    "é«˜ä½æ»æ¶¨é£é™©": 1.0
}

# åœ¨å‡½æ•°å¤–éƒ¨å®šä¹‰å¼ºä¿¡å·åˆ—è¡¨
STRONG_SIGNALS = ["è¶‹åŠ¿çªç ´ç¡®è®¤", "ä¸»åŠ›èµ„é‡‘å…±æŒ¯", "åº•éƒ¨åè½¬ç¡®è®¤"]



POTENTIAL_SIGNALS = [
    "å‡çº¿å¤šå¤´æ’åˆ—", 
    "BOLLä¸­è½¨æ”¯æ’‘", 
    "MACDé‡‘å‰", 
    "çªç ´å‰å¤•"
]
KEY_BUY_SIGNALS = [
    "å›è¸©30æ—¥çº¿ç¡®è®¤",
    "çªç ´5æ—¥çº¿æ”¾é‡",
    "MACDé›¶è½´é‡‘å‰",
    "å‡çº¿å¤šå¤´æ’åˆ—"  # æ–°å¢
]





# =====å„ä¸ªå¯ç”¨æ¥å£åŠå…¶è°ƒç”¨æ–¹æ³• =====
from typing import List
from datetime import datetime, timedelta

# Helperï¼šå¯¹å•ä¸ªæ¥å£çš„æ—¥æœŸå‚æ•°åšå›é€€å°è¯•ï¼Œç›´åˆ°æ‹¿åˆ°éç©ºç»“æœæˆ–ç”¨å°½å¤©æ•°
def _fetch_with_fallback(
    api_func,
    date_field: str,
    base_date: datetime,
    max_back_days: int,
    extra_params: dict
) -> pd.DataFrame:
    for delta in range(max_back_days + 1):
        d = (base_date - timedelta(days=delta)).strftime('%Y%m%d')
        params = extra_params.copy()
        params[date_field] = d
        df = safe_api_call(api_func, **params)
        if not df.empty:
            return df
    return pd.DataFrame()
# ã€è¡Œä¸š&æ¦‚å¿µã€‘åˆå§‹åŒ–æ‰¹é‡ç¼“å­˜
def initialize_industry_and_concept(ts_codes: List[str]):
    global industry_cache, concept_cache
    logger.info("ğŸš€ åˆå§‹åŒ–è¡Œä¸šä¸æ¦‚å¿µç¼“å­˜...")

    # 1. å…¨é‡è·å–è¡Œä¸šä¿¡æ¯ï¼ˆä¸éœ€è¦å›é€€ï¼‰
    try:
        info_df = safe_api_call(
            pro.stock_basic,
            exchange='',
            list_status='L',
            fields='ts_code,industry'
        )
        if not info_df.empty:
            industry_cache.update(
                dict(zip(
                    info_df['ts_code'],
                    info_df['industry'].fillna('æœªçŸ¥è¡Œä¸š')
                ))
            )
            logger.info(f"âœ… è¡Œä¸šä¿¡æ¯ç¼“å­˜å®Œæˆï¼Œå…± {len(industry_cache)} æ¡")
        else:
            logger.warning("âš ï¸ è¡Œä¸šä¿¡æ¯ä¸ºç©º")
    except Exception as e:
        logger.warning(f"è¡Œä¸šæ‰¹é‡è·å–å¤±è´¥: {e}")

    # 2. æ¦‚å¿µæ‰¹é‡ç¼“å­˜ï¼šåˆ†æ‰¹ï¼ˆ200/æ‰¹ï¼‰æŸ¥è¯¢ï¼Œé¿å…å•æ¬¡ä¼ å‚è¿‡é•¿å¯¼è‡´æ¥å£è¿”å›ç©º
    batch_size = 200
    for i in range(0, len(ts_codes), batch_size):
        batch = ts_codes[i : i + batch_size]
        ts_str = ",".join(batch)
        try:
            df = safe_api_call(
                pro.concept_detail,
                ts_code=ts_str,
                fields='ts_code,concept_name'
            )
            if not df.empty:
                grouped = df.groupby('ts_code')['concept_name'].apply(list)
                concept_cache.update(grouped.to_dict())
        except Exception as e:
            logger.warning(f"æ¦‚å¿µæ‰¹é‡è·å–å¤±è´¥ï¼ˆæ‰¹æ¬¡ {i // batch_size + 1}ï¼‰: {e}")

    logger.info(f"âœ… æ¦‚å¿µä¿¡æ¯ç¼“å­˜å®Œæˆï¼Œè¦†ç›–è‚¡ç¥¨æ•°ï¼š{len(concept_cache)}")
        
# ã€å¸‚å€¼æ‰“åˆ†ã€‘å°å¸‚å€¼åŠ åˆ†ï¼Œå¤§å¸‚å€¼æ‰£åˆ†        
def apply_market_cap_penalty(ts_code: str, score: float) -> float:
    try:
        df = safe_api_call(pro.daily_basic, ts_code=ts_code, trade_date=datetime.today().strftime('%Y%m%d'), fields='ts_code,total_mv')
        if df.empty:
            return score
        market_cap = df.iloc[0]['total_mv'] / 10000  # æ¢ç®—ä¸ºäº¿å…ƒ

        if market_cap < 30:
            logger.debug(f"{ts_code} å°å¸‚å€¼ï¼ˆ{market_cap:.1f}äº¿ï¼‰ï¼ŒåŠ 5åˆ†")
            return score + 5
        elif market_cap > 1000:
            penalty = min(10, (market_cap - 1000) * 0.01)
            logger.debug(f"{ts_code} è¶…å¤§å¸‚å€¼ï¼ˆ{market_cap:.1f}äº¿ï¼‰ï¼Œæ‰£{penalty:.1f}åˆ†")
            return score - penalty
        return score
    except Exception as e:
        logger.warning(f"{ts_code} å¸‚å€¼æ‰£åˆ†å¤±è´¥: {str(e)}")
        return score
  
    


# ã€è´¢åŠ¡è¯„åˆ†ã€‘æ ¹æ®ROEã€æ¯›åˆ©ç‡ã€è´Ÿå€ºç‡ç­‰æ‰“åˆ†    
def evaluate_financials(ts_code: str) -> float:
    try:
        # è·å–å½“å‰å¹´ä»½çš„æœ€åä¸€å¤©ï¼ˆ12æœˆ31æ—¥ï¼‰
        current_year = datetime.today().year
        period = f"{current_year - 1}1231"  # ä½¿ç”¨å»å¹´çš„12æœˆ31æ—¥ä½œä¸ºæŠ¥å‘ŠæœŸ

        # ä½¿ç”¨ fina_indicator_vip è·å–è´¢åŠ¡æ•°æ®
        df = safe_api_call(pro.fina_indicator_vip, 
                           ts_code=ts_code, 
                           start_date='20230101',  # è®¾ç½®èµ·å§‹æ—¥æœŸä¸º2023å¹´1æœˆ1æ—¥
                           end_date=datetime.today().strftime('%Y%m%d'),  # æˆªæ­¢åˆ°ä»Šå¤©
                           period=period, 
                           fields='roe,roe_dt,grossprofit_margin,netprofit_yoy,debt_to_assets')
        
        # å¦‚æœæ²¡æœ‰æ•°æ®ï¼Œè®°å½•å¹¶è¿”å› 0
        if df.empty:
            logger.warning(f"{ts_code} æœªè·å–åˆ°è´¢åŠ¡æ•°æ®")
            return 0
        
        # æ‰“å°è°ƒè¯•ä¿¡æ¯ï¼ŒæŸ¥çœ‹è¿”å›çš„æ•°æ®å†…å®¹
        logger.debug(f"{ts_code} è·å–åˆ°çš„è´¢åŠ¡æ•°æ®: {df.head()}")
        
        data = df.iloc[0]  # è·å–ç¬¬ä¸€æ¡æ•°æ®
        score = 0

        # ç¡®ä¿è´¢åŠ¡æ•°æ®ä¸ºæœ‰æ•ˆæ•°å­—ï¼Œå¦åˆ™è®¾ä¸º 0
        roe = data['roe'] if isinstance(data['roe'], (int, float)) else 0
        grossprofit_margin = data['grossprofit_margin'] if isinstance(data['grossprofit_margin'], (int, float)) else 0
        netprofit_yoy = data['netprofit_yoy'] if isinstance(data['netprofit_yoy'], (int, float)) else 0
        debt_to_assets = data['debt_to_assets'] if isinstance(data['debt_to_assets'], (int, float)) else 0

        # æ‰“åˆ†é€»è¾‘
        if roe > 25:
            score += 10  
        elif roe > 20:
            score += 7
        elif roe > 15:
            score += 5
        elif roe > 12:
            score += 3

        if grossprofit_margin > 40:
            score += 7  
        elif grossprofit_margin > 30:
            score += 5
        elif grossprofit_margin > 20:
            score += 3

        if netprofit_yoy > 30:
            score += 7  
        elif netprofit_yoy > 20:
            score += 5
        elif netprofit_yoy > 10:
            score += 3
        elif netprofit_yoy > 0:
            score += 2
        else:
            score -= 10  # è´Ÿå¢é•¿

        if debt_to_assets < 30:
            score += 5  
        elif debt_to_assets < 40:
            score += 3
        elif debt_to_assets > 70:
            score -= 10
        
        # æ–°å¢è·å–ç°é‡‘æµé‡æ•°æ®
        cash_flow = safe_api_call(pro.cashflow_vip, ts_code=ts_code, 
                                  fields="n_cashflow_act, free_cashflow")  # æ›¿æ¢å­—æ®µ
        
        # æ–°å¢ç›ˆåˆ©èƒ½åŠ›è´¨é‡è¯„åˆ†
        profit_quality = 0
        if not cash_flow.empty:
            if cash_flow['n_cashflow_act'].iloc[-1] > 1.2:  # ä½¿ç”¨ n_cashflow_act æ›¿ä»£ ocf_to_operate_income
                profit_quality += 8
            if cash_flow['free_cashflow'].iloc[-1] > 0:
                profit_quality += 6
                
        # æ–°å¢ä¼°å€¼æŒ‡æ ‡
        valuation = safe_api_call(pro.daily_basic, ts_code=ts_code,
                                  fields="pe_ttm,pb")
        val_score = 0
        if not valuation.empty:
            if valuation['pe_ttm'].iloc[-1] < 15:
                val_score += 5
            if valuation['pb'].iloc[-1] < 1.5:
                val_score += 5
        
        # å°†è´¢åŠ¡å¾—åˆ†ä¸æ–°å¢çš„ç›ˆåˆ©èƒ½åŠ›è´¨é‡å’Œä¼°å€¼æŒ‡æ ‡å¾—åˆ†åˆå¹¶
        total_score = score + profit_quality + val_score
        logger.debug(f"{ts_code} è´¢åŠ¡å¥åº·å¾—åˆ†: {total_score}")
        
        return total_score

    except Exception as e:
        logger.debug(f"{ts_code} è´¢åŠ¡æŒ‡æ ‡è·å–å¤±è´¥: {str(e)}")
        return 0






    
# ã€é¾™è™æ¦œã€‘æœºæ„å¸­ä½æ•°æ®ç¼“å­˜    
def initialize_top_inst():
    global top_inst_cache
    logger.info("ğŸš¨ åˆå§‹åŒ–é¾™è™æ¦œæœºæ„å¸­ä½æ•°æ®...")

    try:
        df = safe_api_call(pro.top_inst, trade_date=datetime.today().strftime('%Y%m%d'))
        if not df.empty:
            top_inst_cache = set(df['ts_code'].unique())
            logger.info(f"âœ… æœºæ„å¸­ä½æ•°æ®ç¼“å­˜å®Œæˆï¼š{len(top_inst_cache)} æ”¯è‚¡ç¥¨")
        else:
            top_inst_cache = set()   # æ•°æ®ä¸ºç©ºæ—¶ï¼Œç¡®ä¿æ˜¯ç©ºé›†åˆ
            logger.warning("âš ï¸ æœºæ„å¸­ä½æ•°æ®ä¸ºç©º")
    except Exception as e:
        top_inst_cache = set()       # å¼‚å¸¸æ—¶ä¹Ÿæ¸…ç©ºï¼Œé˜²æ­¢æ®‹ç•™æ—§æ•°æ®
        logger.warning(f"æœºæ„å¸­ä½è·å–å¤±è´¥: {str(e)}")
        
# ã€é¾™è™æ¦œã€‘å‘½ä¸­æœºæ„å¸­ä½åŠ åˆ†
def check_top_inst(ts_code: str) -> float:
    return 10 if ts_code in top_inst_cache else 0

# ã€èµ„é‡‘æµå‘ã€‘è¿‘5æ—¥ä¸»åŠ›èµ„é‡‘å‡€æµå…¥å¾—åˆ†    
def initialize_moneyflow_scores(ts_codes: List[str]):
    global moneyflow_scores
    logger.info("ğŸš€ åˆå§‹åŒ–èµ„é‡‘æµå‘å¾—åˆ†ç¼“å­˜...")

    start_date = (datetime.today() - timedelta(days=5)).strftime('%Y%m%d')
    try:
        # ä¸ä¼  ts_codeï¼Œè·å–è¿‘5æ—¥å…¨å¸‚åœºæ•°æ®
        df = safe_api_call(pro.moneyflow, start_date=start_date)
        if df.empty:
            logger.warning("âš ï¸ èµ„é‡‘æµå‘æ•°æ®ä¸ºç©º")
            return

        # è¿‡æ»¤ç›®æ ‡è‚¡ç¥¨
        df = df[df['ts_code'].isin(ts_codes)]

        # åˆ†ç»„æ±‡æ€»
        grouped = df.groupby('ts_code')['net_mf_amount'].sum()

        # è®¡ç®—å¾—åˆ†
        raw_scores = {}  # ç”¨äºå­˜å‚¨åŸå§‹èµ„é‡‘é¢å¾—åˆ†
        for ts_code in ts_codes:
            net_inflow = grouped.get(ts_code, 0)
            if net_inflow > 0:
                bonus = min(5, net_inflow / 1000)
            else:
                bonus = max(-5, net_inflow / 1000)
            raw_scores[ts_code] = bonus  # å­˜å‚¨åŸå§‹å¾—åˆ†

        # æ ‡å‡†åŒ–å¾—åˆ†ï¼šmin-max æ ‡å‡†åŒ–
        min_score = min(raw_scores.values())
        max_score = max(raw_scores.values())

        # è®¾ç½®ç›®æ ‡å¾—åˆ†èŒƒå›´ [0, 10]
        target_min = 0
        target_max = 10

        # å¯¹æ¯åªè‚¡ç¥¨çš„èµ„é‡‘é¢å¾—åˆ†è¿›è¡Œæ ‡å‡†åŒ–
        for ts_code in ts_codes:
            raw_score = raw_scores[ts_code]
            # ä½¿ç”¨min-maxæ ‡å‡†åŒ–å…¬å¼å°†å¾—åˆ†æ˜ å°„åˆ°[0, 10]çš„èŒƒå›´
            normalized_score = (raw_score - min_score) / (max_score - min_score) * (target_max - target_min) + target_min
            
            # å°†æ ‡å‡†åŒ–åçš„å¾—åˆ†å››èˆäº”å…¥ä¸ºæ•´æ•°
            moneyflow_scores[ts_code] = round(normalized_score)  # å››èˆäº”å…¥ä¸ºæ•´æ•°
        
        logger.info(f"âœ… èµ„é‡‘æµå‘ç¼“å­˜å®Œæˆï¼Œè¦†ç›– {len(moneyflow_scores)} æ”¯è‚¡ç¥¨")
    except Exception as e:
        logger.warning(f"èµ„é‡‘æµå‘æ‰¹é‡è·å–å¤±è´¥: {str(e)}")

        
# ã€èµ„é‡‘æµå‘ã€‘è¿”å›å•ç¥¨èµ„é‡‘å¾—åˆ†        
def evaluate_moneyflow(ts_code: str) -> float:
    return moneyflow_scores.get(ts_code, 0)


# ã€æ¶¨è·Œåœã€‘æ‰¹é‡è·å–æ¶¨è·Œåœä»·æ ¼åŒºé—´
def initialize_stk_limit(ts_codes: List[str]):
    logger.info("ğŸš¨ åˆå§‹åŒ–æ¶¨è·Œåœä»·æ•°æ®...")
    try:
        df = safe_api_call(pro.stk_limit, trade_date=datetime.today().strftime('%Y%m%d'))
        if not df.empty:
            filtered = df[df['ts_code'].isin(ts_codes)]
            stk_limit_cache.update(filtered.set_index('ts_code')[['up_limit', 'down_limit']].to_dict('index'))
            logger.info(f"âœ… æ¶¨è·Œåœä»·ç¼“å­˜å®Œæˆï¼š{len(stk_limit_cache)} æ”¯è‚¡ç¥¨")
        else:
            logger.warning("âš ï¸ æ¶¨è·Œåœæ•°æ®ä¸ºç©º")
    except Exception as e:
        logger.warning(f"æ¶¨è·Œåœä»·è·å–å¤±è´¥: {str(e)}")

# ã€å¤§å®—äº¤æ˜“ã€‘ç»Ÿè®¡è¿‘30æ—¥æ´»è·ƒåº¦ï¼Œé€‚å½“åŠ åˆ†        
def initialize_block_trade(ts_codes: List[str]):
    logger.info("ğŸš¨ åˆå§‹åŒ–å¤§å®—äº¤æ˜“æ•°æ®...")
    try:
        start_date = (datetime.today() - timedelta(days=30)).strftime('%Y%m%d')
        df = safe_api_call(pro.block_trade, start_date=start_date)
        if not df.empty:
            filtered = df[df['ts_code'].isin(ts_codes)]
            grouped = filtered.groupby('ts_code').size()
            block_trade_cache.update(grouped.to_dict())
            logger.info(f"âœ… å¤§å®—äº¤æ˜“ç¼“å­˜å®Œæˆï¼š{len(block_trade_cache)} æ”¯è‚¡ç¥¨")
        else:
            logger.warning("âš ï¸ å¤§å®—äº¤æ˜“æ•°æ®ä¸ºç©º")
    except Exception as e:
        logger.warning(f"å¤§å®—äº¤æ˜“è·å–å¤±è´¥: {str(e)}")


def save_to_local_cache(data, filename):
    """å°†æ•°æ®ä¿å­˜åˆ°æœ¬åœ°ç¼“å­˜æ–‡ä»¶"""
    with open(filename, 'w') as f:
        json.dump(data, f, ensure_ascii=False, indent=4)

def load_from_local_cache(filename):
    """ä»æœ¬åœ°ç¼“å­˜æ–‡ä»¶åŠ è½½æ•°æ®"""
    if os.path.exists(filename):
        with open(filename, 'r') as f:
            return json.load(f)
    return None

def fetch_stock_data(ts_code, trade_date_str, retries=3, timeout=30):
    """è·å–æ¯ä¸ªæ¦‚å¿µæ¿å—çš„æ•°æ®ï¼Œå¹¶å¤„ç†è¶…æ—¶"""
    for attempt in range(retries):
        try:
            stock_data = StockAnalyzer.pro.daily(ts_code=ts_code, trade_date=trade_date_str, fields='ts_code,close', timeout=timeout)
            if not stock_data.empty:
                return stock_data.to_dict(orient='records')
            else:
                logger.warning(f"è·å– {ts_code} çš„æ•°æ®ä¸ºç©º")
                return None
        except Exception as e:
            logger.warning(f"è·å– {ts_code} çš„æ•°æ®æ—¶å‡ºé”™: {e}")
            if attempt < retries - 1:
                logger.info(f"é‡è¯• {ts_code} (å°è¯• {attempt + 1} æ¬¡)")
                time.sleep(1)
    return None

def get_concept_trends(trade_date: str):
    """
    è·å–åŒèŠ±é¡ºæ¦‚å¿µæ¿å—æ•°æ®å¹¶è®¡ç®—æ¯ä¸ªæ¦‚å¿µæ¿å—çš„è¶‹åŠ¿ã€‚
    """
    if IS_BACKTEST:  # å¦‚æœæ˜¯å›æµ‹æ¨¡å¼ï¼Œè·³è¿‡è·å–åŒèŠ±é¡ºæ¦‚å¿µæ•°æ®
        logger.info("å›æµ‹æ¨¡å¼ä¸‹ï¼Œè·³è¿‡è·å–åŒèŠ±é¡ºæ¦‚å¿µæ•°æ®")
        return pd.DataFrame()  # è¿”å›ä¸€ä¸ªç©ºçš„DataFrame

    cache_filename = f'concept_data_{trade_date}.json'  # ä½¿ç”¨æ—¥æœŸä½œä¸ºæ–‡ä»¶åçš„ä¸€éƒ¨åˆ†æ¥åŒºåˆ†ä¸åŒçš„ç¼“å­˜

    # å°è¯•ä»æœ¬åœ°ç¼“å­˜åŠ è½½æ•°æ®
    cached_data = load_from_local_cache(cache_filename)
    if cached_data:
        logger.info(f"åŠ è½½æœ¬åœ°ç¼“å­˜æ•°æ®ï¼Œå…±{len(cached_data)}ä¸ªæ¦‚å¿µæ¿å—")
        return cached_data

    try:
        # å°† trade_date è½¬æ¢ä¸ºå­—ç¬¦ä¸²æ ¼å¼
        trade_date_str = trade_date.strftime('%Y%m%d') if isinstance(trade_date, datetime) else trade_date

        # è·å–æ‰€æœ‰æ¦‚å¿µæ¿å—æ•°æ®ï¼ˆä¸€æ¬¡æ€§è·å–ï¼Œå°½é‡é¿å…åˆ†é¡µï¼‰
        df = StockAnalyzer.pro.ths_index(type='N')  # è·å–æ¦‚å¿µæŒ‡æ•°ï¼Œtype='N'è¡¨ç¤ºæ¦‚å¿µæŒ‡æ•°

        if df is not None and not df.empty:
            df = df[['ts_code', 'name', 'count']]  # åªæå–è‚¡ç¥¨ä»£ç ã€æ¿å—åç§°å’Œæˆåˆ†è‚¡æ•°
            logger.info(f"è·å–åŒèŠ±é¡ºæ¦‚å¿µæ¿å—æ•°æ®æˆåŠŸï¼Œå…±{len(df)}ä¸ªæ¿å—")

            # å°†æ‰€æœ‰æ¦‚å¿µæ¿å—çš„æ¶¨å¹…æ•°æ®ä¸€æ¬¡æ€§è·å–
            concept_data = []

            # ä½¿ç”¨å¹¶å‘å¤„ç†è·å–æ¯ä¸ªæ¿å—çš„æ¶¨å¹…æ•°æ®
            def fetch_stock_data_for_row(row):
                """å°è£… row è·å–æ•°æ®çš„é€»è¾‘"""
                ts_code = row['ts_code']
                result = fetch_stock_data(ts_code, trade_date_str)
                return result

            with concurrent.futures.ThreadPoolExecutor() as executor:
                future_to_ts_code = {executor.submit(fetch_stock_data_for_row, row): row for _, row in df.iterrows()}

                # ä½¿ç”¨ tqdm æ˜¾ç¤ºè¿›åº¦æ¡
                for future in tqdm(concurrent.futures.as_completed(future_to_ts_code), total=len(future_to_ts_code), desc="è·å–æ¦‚å¿µæ¿å—æ¶¨å¹…æ•°æ®"):
                    result = future.result()
                    if result:
                        concept_data.append(result)

            logger.info(f"æˆåŠŸè·å– {len(concept_data)} ä¸ªæ¦‚å¿µæ¿å—çš„æ¶¨å¹…æ•°æ®")

            # ç¼“å­˜æ•°æ®åˆ°æœ¬åœ°
            save_to_local_cache(concept_data, cache_filename)

            # è½¬æ¢ä¸º DataFrame
            concept_data_df = pd.DataFrame(concept_data)
            return concept_data_df
        else:
            logger.warning("æ— æ³•è·å–åŒèŠ±é¡ºæ¦‚å¿µæ¿å—æ•°æ®")
    except Exception as e:
        logger.error(f"è·å–åŒèŠ±é¡ºæ¦‚å¿µæ¿å—æ•°æ®å‡ºé”™: {str(e)}")
    return pd.DataFrame()

def calculate_concept_trend_score(concept_data: pd.DataFrame, trade_date: str):
    """
    æ ¹æ®æ¦‚å¿µæ¿å—çš„æ•°æ®ï¼Œè®¡ç®—æ¯ä¸ªæ¦‚å¿µçš„è¶‹åŠ¿è¯„åˆ†ã€‚
    """
    total_pct_change = 0
    valid_count = 0

    # ç¡®ä¿ concept_data æ˜¯ DataFrame æ ¼å¼
    if isinstance(concept_data, list):
        concept_data = pd.DataFrame(concept_data)

    trade_date_str = trade_date.strftime('%Y%m%d') if isinstance(trade_date, datetime) else trade_date

    for index, row in tqdm(concept_data.iterrows(), total=len(concept_data), desc="è®¡ç®—æ¦‚å¿µæ¿å—è¶‹åŠ¿å¾—åˆ†"):
        concept_ts_code = row['ts_code']
        concept_name = row['name']
        
        # æ£€æŸ¥æ˜¯å¦æœ‰ç¼“å­˜çš„è¶‹åŠ¿è¯„åˆ†
        cached_score = load_trend_score_from_cache(concept_name)
        if cached_score:
            logger.info(f"åŠ è½½ç¼“å­˜çš„è¶‹åŠ¿è¯„åˆ†ï¼š{concept_name} - {cached_score['trend_score']}")
            total_pct_change += cached_score['trend_score']
            valid_count += 1
            continue
        
        # è·å–æ¿å—ä¸­æ¯åªè‚¡ç¥¨çš„æ¶¨å¹…
        try:
            stock_data = StockAnalyzer.pro.daily(ts_code=concept_ts_code, trade_date=trade_date_str, fields='ts_code,close')
            if not stock_data.empty:
                pct_change = stock_data['pct_chg'].mean()  # è®¡ç®—å¹³å‡æ¶¨å¹…
                total_pct_change += pct_change
                valid_count += 1
                # ç¼“å­˜è®¡ç®—çš„è¶‹åŠ¿è¯„åˆ†
                save_trend_score_to_cache(concept_name, pct_change)
        except Exception as e:
            logger.warning(f"è·å– {concept_name} æ¿å—çš„æ¶¨å¹…æ•°æ®å‡ºé”™: {str(e)}")

    # è¿”å›è®¡ç®—åçš„è¶‹åŠ¿è¯„åˆ†
    if valid_count > 0:
        avg_pct_change = total_pct_change / valid_count
        logger.info(f"æ¦‚å¿µæ¿å—å¹³å‡æ¶¨å¹…: {avg_pct_change:.2f}%")
        return avg_pct_change
    return 0.0


def save_trend_score_to_cache(concept_name, trend_score):
    """ä¿å­˜è¶‹åŠ¿è¯„åˆ†åˆ°ç¼“å­˜"""
    filename = f'trend_score_{concept_name}.json'
    data = {'concept_name': concept_name, 'trend_score': trend_score}
    save_to_local_cache(data, filename)

def load_trend_score_from_cache(concept_name):
    """åŠ è½½ç¼“å­˜çš„è¶‹åŠ¿è¯„åˆ†"""
    filename = f'trend_score_{concept_name}.json'
    return load_from_local_cache(filename)






SECTOR_SCORE_CACHE = "sector_scores_cache.json"
CONCEPT_STOCK_MAP_FILE = "concept_to_stock_map.json"
CONCEPT_NAME_TO_CODE_FILE = "concept_name_to_code.json"

# === è·å–æ¦‚å¿µæ¿å—çƒ­åº¦è¯„åˆ† ===
def get_sector_strength_scores(trade_date: str) -> Dict[str, float]:
    """æ ¹æ®é€šè¾¾ä¿¡æ¿å—æ•°æ®æ‰“åˆ†æ¯ä¸ªæ¦‚å¿µæ¿å—"""
        # å›æµ‹æ¨¡å¼ä¸‹ï¼Œè·³è¿‡æ¿å—è¯„åˆ†
    if IS_BACKTEST:
        logger.info(f"ğŸ”™ å›æµ‹æ¨¡å¼ï¼šè·³è¿‡é€šè¾¾ä¿¡æ¿å—è¯„åˆ†")
        return {}
    logger.info(f"ğŸ” å¼€å§‹è·å– {trade_date} çš„æ¿å—æ•°æ®...")
    
    try:
        # ä½¿ç”¨ safe_api_call ç¡®ä¿ä¸€è‡´çš„é”™è¯¯å¤„ç†
        df = safe_api_call(
            pro.tdx_daily,
            trade_date=trade_date,
            fields="ts_code,3day,5day,bm_buy_ratio,turnover_rate,idx_type"
        )
        
        if df.empty:
            logger.warning(f"âš ï¸ {trade_date} æ— æ¿å—æ•°æ®è¿”å›")
            return {}
            
        logger.info(f"ğŸ“Š åŸå§‹æ¿å—æ•°æ®æ•°é‡: {len(df)}")
        
        # æ£€æŸ¥è¿”å›çš„å­—æ®µ
        logger.info(f"ğŸ“Š è¿”å›çš„å­—æ®µ: {df.columns.tolist()}")
        
        # æ£€æŸ¥idx_typeå­—æ®µæ˜¯å¦å­˜åœ¨
        if 'idx_type' not in df.columns:
            logger.error(f"âŒ è¿”å›æ•°æ®ç¼ºå°‘ idx_type å­—æ®µï¼Œå°è¯•å…¶ä»–æ–¹æ³•")
            # å°è¯•è·å–æ‰€æœ‰é€šè¾¾ä¿¡æ¦‚å¿µæ¿å—ä»£ç ï¼ˆä»¥880å¼€å¤´çš„ä»£ç ï¼‰
            df_filtered = df[df['ts_code'].str.startswith('880')]
            logger.info(f"ğŸ“Š ä½¿ç”¨ä»£ç å‰ç¼€ç­›é€‰ï¼Œæ‰¾åˆ° {len(df_filtered)} ä¸ªæ¿å—")
        else:
            logger.info(f"ğŸ“Š åŒ…å«çš„æ¿å—ç±»å‹: {df['idx_type'].unique()}")
            # ç­›é€‰æ¦‚å¿µæ¿å—
            df_filtered = df[df['idx_type'] == 'æ¦‚å¿µæ¿å—']
            logger.info(f"ğŸ“Š ç­›é€‰åæ¦‚å¿µæ¿å—æ•°é‡: {len(df_filtered)}")
        
        if df_filtered.empty:
            logger.warning("âš ï¸ æ²¡æœ‰æ‰¾åˆ°æ¦‚å¿µæ¿å—æ•°æ®")
            return {}
            
    except Exception as e:
        logger.error(f"âŒ æ¿å—è¯„åˆ†è·å–å¤±è´¥ï¼š{e}")
        logger.error(f"é”™è¯¯è¯¦æƒ…: {type(e).__name__}: {str(e)}")
        import traceback
        logger.error(f"å †æ ˆä¿¡æ¯:\n{traceback.format_exc()}")
        return {}
    
    scores = {}
    for _, row in df_filtered.iterrows():
        try:
            # æ£€æŸ¥å¿…è¦å­—æ®µæ˜¯å¦å­˜åœ¨
            required_fields = ['3day', '5day', 'bm_buy_ratio', 'turnover_rate']
            missing_fields = [f for f in required_fields if f not in row or pd.isna(row[f])]
            
            if missing_fields:
                logger.warning(f"âš ï¸ æ¿å— {row['ts_code']} ç¼ºå°‘å­—æ®µ: {missing_fields}")
                continue
            
            score = (
                row['3day'] * 0.2 +
                row['5day'] * 0.3 +
                row['bm_buy_ratio'] * 0.3 +
                row['turnover_rate'] * 0.2
            )
            scores[row['ts_code']] = round(score, 2)
        except Exception as e:
            logger.warning(f"âš ï¸ è®¡ç®—æ¿å— {row['ts_code']} è¯„åˆ†å¤±è´¥: {e}")
            continue
    
    logger.info(f"âœ… æˆåŠŸè®¡ç®— {len(scores)} ä¸ªæ¿å—çš„è¯„åˆ†")
    if scores:
        # æ‰“å°è¯„åˆ†æœ€é«˜çš„5ä¸ªæ¿å—
        top_5 = sorted(scores.items(), key=lambda x: x[1], reverse=True)[:5]
        logger.info(f"ğŸ† è¯„åˆ†TOP5æ¿å—: {top_5}")
    
    # å¯é€‰ï¼šå†™å…¥ç¼“å­˜
    try:
        with open(SECTOR_SCORE_CACHE, 'w', encoding='utf-8') as f:
            json.dump(scores, f, ensure_ascii=False, indent=2)
        logger.info(f"ğŸ’¾ æ¿å—è¯„åˆ†å·²ç¼“å­˜åˆ° {SECTOR_SCORE_CACHE}")
    except Exception as e:
        logger.warning(f"âš ï¸ æ¿å—è¯„åˆ†ç¼“å­˜å¤±è´¥: {e}")
    
    return scores

# === è‚¡ç¥¨æ‰€å±æ¦‚å¿µæ˜ å°„è¡¨ ===
def load_concept_to_stock_map() -> Dict[str, List[str]]:
    logger.info(f"ğŸ“š å¼€å§‹åŠ è½½æ¦‚å¿µè‚¡ç¥¨æ˜ å°„æ–‡ä»¶: {CONCEPT_STOCK_MAP_FILE}")
    
    if os.path.exists(CONCEPT_STOCK_MAP_FILE):
        try:
            with open(CONCEPT_STOCK_MAP_FILE, 'r', encoding='utf-8') as f:
                data = json.load(f)
            logger.info(f"âœ… æˆåŠŸåŠ è½½ {len(data)} ä¸ªæ¦‚å¿µçš„è‚¡ç¥¨æ˜ å°„")
            
            # æ‰“å°ä¸€äº›ç»Ÿè®¡ä¿¡æ¯
            total_stocks = sum(len(stocks) for stocks in data.values())
            logger.info(f"ğŸ“Š æ€»è®¡åŒ…å« {total_stocks} ä¸ªè‚¡ç¥¨æ˜ å°„å…³ç³»")
            
            # æ‰“å°å‰3ä¸ªæ¦‚å¿µçš„ä¿¡æ¯
            for i, (concept, stocks) in enumerate(data.items()):
                if i >= 3:
                    break
                logger.debug(f"  æ¦‚å¿µæ ·ä¾‹ {i+1}: {concept} -> {len(stocks)} åªè‚¡ç¥¨")
            
            return data
        except Exception as e:
            logger.error(f"âŒ åŠ è½½æ¦‚å¿µè‚¡ç¥¨æ˜ å°„å¤±è´¥: {e}")
            return {}
    else:
        logger.warning(f"âš ï¸ æ¦‚å¿µè‚¡ç¥¨æ˜ å°„æ–‡ä»¶ä¸å­˜åœ¨: {CONCEPT_STOCK_MAP_FILE}")
        return {}

def load_concept_name_to_code() -> Dict[str, str]:
    """åŠ è½½æ¦‚å¿µåç§°åˆ°ä»£ç çš„æ˜ å°„"""
    logger.info(f"ğŸ”— å¼€å§‹åŠ è½½æ¦‚å¿µä»£ç æ˜ å°„æ–‡ä»¶: {CONCEPT_NAME_TO_CODE_FILE}")
    
    if os.path.exists(CONCEPT_NAME_TO_CODE_FILE):
        try:
            with open(CONCEPT_NAME_TO_CODE_FILE, 'r', encoding='utf-8') as f:
                data = json.load(f)
            logger.info(f"âœ… æˆåŠŸåŠ è½½ {len(data)} ä¸ªæ¦‚å¿µä»£ç æ˜ å°„")
            
            # æ‰“å°å‰5ä¸ªæ˜ å°„æ ·ä¾‹
            sample_items = list(data.items())[:5]
            for name, code in sample_items:
                logger.debug(f"  æ˜ å°„æ ·ä¾‹: {name} -> {code}")
            
            return data
        except Exception as e:
            logger.error(f"âŒ åŠ è½½æ¦‚å¿µä»£ç æ˜ å°„å¤±è´¥: {e}")
            return {}
    else:
        logger.warning(f"âš ï¸ æ¦‚å¿µåç§°æ˜ å°„æ–‡ä»¶ä¸å­˜åœ¨: {CONCEPT_NAME_TO_CODE_FILE}")
        return {}

def get_stock_concepts(stock_code: str, concept_map: Dict[str, List[str]]) -> List[str]:
    """è·å–æŸä¸ªè‚¡ç¥¨æ‰€å±æ¦‚å¿µåç§°åˆ—è¡¨"""
    result = []
    for concept, stocks in concept_map.items():
        if stock_code in stocks:
            result.append(concept)
    
    if result:
        logger.debug(f"ğŸ·ï¸ {stock_code} æ‰€å±æ¦‚å¿µ: {result}")
    else:
        logger.debug(f"âš ï¸ {stock_code} æœªæ‰¾åˆ°æ‰€å±æ¦‚å¿µ")
    
    return result

def inject_sector_score(final_score: float, stock_code: str, 
                        concept_name_to_ts_code: Dict[str, str], 
                        sector_scores: Dict[str, float], 
                        concept_map: Dict[str, List[str]],
                        weight: float = 0.3) -> float:
    """å¯¹å•åªè‚¡ç¥¨ final_score æ·»åŠ æ¿å—çƒ­åº¦åŠ æˆ"""
    # å›æµ‹æ¨¡å¼ä¸‹ç›´æ¥è¿”å›åŸåˆ†æ•°
    if IS_BACKTEST:
        logger.debug(f"ğŸ”™ å›æµ‹æ¨¡å¼ï¼š{stock_code} è·³è¿‡æ¿å—çƒ­åº¦åŠ åˆ†")
        return final_score
    
    # å¦‚æœæ²¡æœ‰æ¿å—è¯„åˆ†æ•°æ®ï¼Œä¹Ÿç›´æ¥è¿”å›
    if not sector_scores:
        logger.debug(f"âš ï¸ æ— æ¿å—è¯„åˆ†æ•°æ®ï¼Œ{stock_code} è·³è¿‡æ¿å—çƒ­åº¦åŠ åˆ†")
        return final_score
    logger.debug(f"ğŸ’‰ å¼€å§‹è®¡ç®— {stock_code} çš„æ¿å—çƒ­åº¦åŠ åˆ†...")
    
    concepts = get_stock_concepts(stock_code, concept_map)
    if not concepts:
        logger.debug(f"âš ï¸ {stock_code} æ— æ‰€å±æ¦‚å¿µï¼Œæ¿å—åŠ åˆ†ä¸º0")
        return final_score
    
    logger.debug(f"ğŸ·ï¸ {stock_code} æ‰€å±æ¦‚å¿µ: {concepts}")
    
    score_boost = 0.0
    detail_boosts = []
    
    for c in concepts:
        ts_code = concept_name_to_ts_code.get(c)
        logger.debug(f"ğŸ” æ¦‚å¿µ '{c}' å¯¹åº”ä»£ç : {ts_code}")
        
        if ts_code:
            sector_score = sector_scores.get(ts_code, 0.0)
            logger.debug(f"ğŸ“Š {c} ({ts_code}) æ¿å—è¯„åˆ†: {sector_score}")
            score_boost += sector_score
            detail_boosts.append(f"{c}({sector_score})")
        else:
            logger.debug(f"âš ï¸ æ¦‚å¿µ '{c}' æœªæ‰¾åˆ°å¯¹åº”ä»£ç ")
    
    weighted_boost = score_boost * weight
    logger.debug(f"ğŸ’° {stock_code} æ¿å—çƒ­åº¦æ€»åˆ†: {score_boost:.2f} Ã— {weight} = {weighted_boost:.2f}")
    logger.debug(f"ğŸ“Š å„æ¿å—è´¡çŒ®: {', '.join(detail_boosts)}")
    
    return final_score + weighted_boost








# æ–°å¢å¤åˆä¿¡å·æ£€æµ‹å‡½æ•°
def detect_composite_signals(df: pd.DataFrame) -> dict:
    signals = {}
    
    # ä¸šç»©çªç ´+é‡ä»·é½å‡
    earnings_growth = (df['revenue_yoy'].iloc[-1] > 0.3) & (df['netprofit_yoy'].iloc[-1] > 0.5)
    volume_spike = df['vol'].iloc[-1] > df['vol'].rolling(20).mean().iloc[-1] * 1.5
    signals["ä¸šç»©çªç ´+é‡ä»·é½å‡"] = earnings_growth & volume_spike
    
    # æœºæ„å¢æŒ+ä¼°å€¼ä¿®å¤
    inst_holding = (df['holder_num'].iloc[-1] < df['holder_num'].iloc[-2]) & \
                  (df['inst_holding_ratio'].iloc[-1] > df['inst_holding_ratio'].iloc[-2])
    valuation_improve = (df['pe_ttm'].iloc[-1] < df['pe_ttm'].rolling(20).mean().iloc[-1]) & \
                       (df['pb'].iloc[-1] < df['pb'].rolling(20).mean().iloc[-1])
    signals["æœºæ„å¢æŒ+ä¼°å€¼ä¿®å¤"] = inst_holding & valuation_improve
    
    return signals










def initialize_share_float_data(ts_codes: List[str], days_ahead: int = 30):
    """ç»Ÿè®¡æœªæ¥ days_ahead å¤©å†…å°†è¦è§£ç¦çš„è‚¡ä»½æ€»é‡"""
    logger.info(f"ğŸš¨ åˆå§‹åŒ–é™å”®è§£ç¦æ•°æ®ï¼ˆæœªæ¥ {days_ahead} å¤©ï¼‰...")
    batch_size = 200
    today = datetime.today()
    end_date = (today + timedelta(days=days_ahead)).strftime('%Y%m%d')
    today_str = today.strftime('%Y%m%d')

    # é¢„å¡«å……é»˜è®¤å€¼
    for code in ts_codes:
        share_float_cache.setdefault(code, 0)

    for i in range(0, len(ts_codes), batch_size):
        batch = ts_codes[i:i + batch_size]
        ts_str = ",".join(batch)

        try:
            # è·å–ä»Šå¤©å…¬å‘Šçš„æ‰€æœ‰è§£ç¦è®¡åˆ’
            df = safe_api_call(pro.share_float, ts_code=ts_str, ann_date=today_str)

            if df.empty or 'ts_code' not in df.columns:
                logger.warning(f"âš ï¸ share_float æ‰¹æ¬¡ {i // batch_size + 1} æ•°æ®ä¸ºç©ºæˆ–å­—æ®µç¼ºå¤±")
                continue

            # ç»Ÿè®¡æœªæ¥ N å¤©å†…å°†è§£ç¦çš„æ€»è‚¡æ•°
            valid_codes = df['ts_code'].unique()
            for code in batch:
                if code in valid_codes:
                    sub_df = df[(df['ts_code'] == code) &
                                (df['float_date'] >= today_str) &
                                (df['float_date'] <= end_date)]
                    future_unlock = sub_df['float_share'].sum()
                    share_float_cache[code] = future_unlock
                    logger.info(f"âœ… {code} æœªæ¥{days_ahead}å¤©å°†è§£ç¦ï¼š{future_unlock:.1f} ä¸‡è‚¡")
                else:
                    share_float_cache[code] = 0

        except Exception as e:
            logger.error(f"âŒ share_float æ‰¹æ¬¡ {i // batch_size + 1} å¤„ç†å¤±è´¥ï¼š{str(e)}")
            for code in batch:
                share_float_cache[code] = 0

    logger.info(f"âœ… é™å”®è§£ç¦æ•°æ®åˆå§‹åŒ–å®Œæˆ | æœ‰æ•ˆè‚¡ç¥¨æ•°ï¼š{len([v for v in share_float_cache.values() if v > 0])}")


def evaluate_share_float(ts_code: str) -> float:
    """æœªæ¥è§£ç¦è¯„åˆ†"""
    try:
        unlock_amount = share_float_cache.get(ts_code, 0)

        if not isinstance(unlock_amount, (int, float)):
            logger.warning(f"{ts_code} è§£ç¦æ•°æ®å¼‚å¸¸ï¼š{unlock_amount}ï¼ŒæŒ‰0å¤„ç†")
            return 0

        # æœªæ¥è§£ç¦é£é™©è¯„åˆ†
        if unlock_amount > 10000:
            logger.debug(f"{ts_code} æœªæ¥è§£ç¦ {unlock_amount:.1f} ä¸‡è‚¡ï¼Œæ‰£6åˆ†")
            return -6
        elif unlock_amount > 5000:
            return -4
        elif unlock_amount > 2000:
            return -2
        return 0

    except Exception as e:
        logger.error(f"{ts_code} è§£ç¦è¯„åˆ†å¤±è´¥ï¼š{str(e)}")
        return 0


def initialize_holdernumber_data(ts_codes: List[str]):
    logger.info("ğŸš¨ åˆå§‹åŒ–è‚¡ä¸œäººæ•°æ•°æ®...")
    batch_size = 200
    today = datetime.today()
    one_year_ago = (today - timedelta(days=365)).strftime('%Y%m%d')

    uncached_codes = [code for code in ts_codes if code not in holdernumber_cache]

    for i in range(0, len(uncached_codes), batch_size):
        batch = uncached_codes[i:i + batch_size]
        ts_str = ",".join(batch)

        try:
            # è·å–è‚¡ä¸œäººæ•°æ•°æ®
            df = safe_api_call(pro.stk_holdernumber, ts_code=ts_str, start_date=one_year_ago, end_date=today.strftime('%Y%m%d'))

            if df.empty or 'ts_code' not in df.columns:
                logger.warning(f"âš ï¸ æ‰¹æ¬¡ {i // batch_size + 1} è‚¡ä¸œäººæ•°æ•°æ®ä¸ºç©ºæˆ–å­—æ®µç¼ºå¤±")
                continue

            # è®¡ç®—è‚¡ä¸œäººæ•°å˜åŒ–
            for code in batch:
                sub_df = df[df['ts_code'] == code].sort_values('end_date', ascending=False)
                if len(sub_df) < 2:
                    holdernumber_cache[code] = 0
                else:
                    latest, prev = sub_df.iloc[0]['holder_num'], sub_df.iloc[1]['holder_num']
                    holdernumber_cache[code] = prev - latest

        except Exception as e:
            logger.error(f"è‚¡ä¸œäººæ•°æ•°æ®å¤„ç†å¤±è´¥ï¼ˆæ‰¹æ¬¡ {i // batch_size + 1}ï¼‰: {str(e)}")
            for code in batch:
                holdernumber_cache[code] = 0

    logger.info(f"âœ… è‚¡ä¸œäººæ•°æ•°æ®ç¼“å­˜å®Œæˆï¼šæœ‰æ•ˆè‚¡ç¥¨æ•° {len(holdernumber_cache)}")


def evaluate_holdernumber(ts_code: str) -> float:
    diff = holdernumber_cache.get(ts_code, 0)
    if diff > 500:
        logger.debug(f"{ts_code} è‚¡ä¸œäººæ•°å¤§å¹…å‡å°‘ {diff}ï¼ŒåŠ 5åˆ†")
        return 5
    elif diff > 100:
        return 3
    return 0


def initialize_express_data(period: str, ts_codes: List[str]):
    logger.info("ğŸš¨ åˆå§‹åŒ–ä¸šç»©å¿«æŠ¥æ•°æ®...")

    try:
        df = safe_api_call(pro.express_vip, period=period, fields='ts_code,ann_date,end_date,revenue,operate_profit,total_profit,n_income,total_assets')

        if df is None or df.empty:
            logger.warning(f"âš ï¸ {period} çš„ä¸šç»©å¿«æŠ¥æ•°æ®ä¸ºç©º")
            return

        for index, row in df.iterrows():
            ts_code = row['ts_code']
            profit_yoy = row.get('net_profit_yoy', 0) or row.get('yoy_net_profit', 0) or row.get('yoy_sales', 0)
            express_cache[ts_code] = profit_yoy
            logger.info(f"{ts_code} ä¸šç»©å¿«æŠ¥å‡€åˆ©åŒæ¯”ï¼š{profit_yoy:.1f}%")

        logger.info(f"âœ… ä¸šç»©å¿«æŠ¥æ•°æ®ç¼“å­˜å®Œæˆï¼š{len(express_cache)} æ”¯è‚¡ç¥¨")

    except Exception as e:
        logger.error(f"è·å–ä¸šç»©å¿«æŠ¥æ•°æ®å¤±è´¥ for {period}: {e}")


def evaluate_express(ts_code: str) -> float:
    profit_yoy = express_cache.get(ts_code, 0)
    if profit_yoy > 50:
        logger.debug(f"{ts_code} ä¸šç»©å¿«æŠ¥å‡€åˆ©åŒæ¯” +{profit_yoy:.1f}%ï¼ŒåŠ 6åˆ†")
        return 6
    elif profit_yoy > 20:
        return 4
    elif profit_yoy > 0:
        return 2
    elif profit_yoy < -30:
        return -4
    return 0



def initialize_risk_data(ts_codes: List[str]):
    today = datetime.today()
    batch_size = 200
    three_years_ago = (today - timedelta(days=3*365)).strftime('%Y%m%d')
    one_year_ago = (today - timedelta(days=365)).strftime('%Y%m%d')

    # ç»Ÿä¸€ä¸ºçŸ­ä»£ç æ ¼å¼ï¼ˆå¦‚600000ï¼‰
    ts_codes = [c.split('.')[0] for c in list(set(ts_codes))]
    uncached_codes = [
        c for c in ts_codes
        if c not in pledge_stat_cache 
        or c not in pledge_detail_cache 
        or c not in holder_trade_cache
    ]

    if not uncached_codes:
        logger.info("âœ… é£é™©æ•°æ®å·²å…¨ç¼“å­˜ï¼Œè·³è¿‡åˆå§‹åŒ–")
        return

    for i in range(0, len(uncached_codes), batch_size):
        batch = uncached_codes[i:i + batch_size]
        # è¡¥å…¨äº¤æ˜“æ‰€ä»£ç ï¼ˆ600000 -> 600000.SHï¼‰
        ts_str = ",".join([
            f"{c}.SH" if c.startswith(('6', '9')) else f"{c}.SZ" 
            for c in batch
        ])

        # ================== è´¨æŠ¼ç‡ç»Ÿè®¡ ==================
        try:
            # ä»èµ„äº§è´Ÿå€ºè¡¨è·å–æœ€æ–°è´¨æŠ¼ç‡
            df_stat = safe_api_call(
                pro.balancesheet,
                ts_code=ts_str,
                period='20231231',  # ä½¿ç”¨æœ€æ–°å¹´æŠ¥
                fields='ts_code,pledge_ratio'
            )
            if not df_stat.empty:
                for _, row in df_stat.iterrows():
                    code = row['ts_code'].split('.')[0]
                    pledge_stat_cache[code] = float(row['pledge_ratio'])
        except Exception as e:
            logger.error(f"è´¨æŠ¼ç‡è·å–å¤±è´¥ï¼š{str(e)}")

        # ================== è´¨æŠ¼æ¬¡æ•°ç»Ÿè®¡ ==================
        try:
            df_pledge = pd.DataFrame()
            three_years_ago = (today - timedelta(days=3*365)).strftime('%Y%m%d')
            for offset in range(0, 5000, 1000):
                chunk = safe_api_call(
                    pro.pledge_detail,
                    ts_code=ts_str,
                    limit=1000,
                    offset=offset
                )
                df_pledge = pd.concat([df_pledge, chunk])
            
            if not df_pledge.empty:
                df_pledge['base_code'] = df_pledge['ts_code'].str.split('.').str[0]
                df_pledge = df_pledge[df_pledge['ann_date'] >= three_years_ago]
                df_pledge = df_pledge.drop_duplicates(['base_code', 'ann_date'])
                pledge_counts = df_pledge.groupby('base_code').size()
                
                for code in batch:
                    pledge_detail_cache[code] = pledge_counts.get(code, 0)
        except Exception as e:
            logger.error(f"è´¨æŠ¼æ¬¡æ•°è·å–å¤±è´¥ï¼š{str(e)}")

        # ================== å‡æŒæ¬¡æ•°ç»Ÿè®¡ ==================
        try:
            df_holders = safe_api_call(
                pro.top10_holders,
                ts_code=ts_str,
                start_date=one_year_ago,
                end_date=today.strftime('%Y%m%d'),
                fields='ts_code,hold_change'
            )
            if not df_holders.empty:
                df_holders['hold_change'] = pd.to_numeric(df_holders['hold_change'], errors='coerce')
                reduce_df = df_holders[df_holders['hold_change'] < 0]
                reduce_counts = reduce_df.groupby('ts_code').size()
                
                for code in batch:
                    short_code = code.split('.')[0] if '.' in code else code
                    holder_trade_cache[code] = reduce_counts.get(f"{short_code}.SH", reduce_counts.get(f"{short_code}.SZ", 0))
        except Exception as e:
            logger.error(f"å‡æŒæ¬¡æ•°è·å–å¤±è´¥ï¼š{str(e)}")

    logger.info(f"âœ… é£é™©æ•°æ®åˆå§‹åŒ–å®Œæˆ | è´¨æŠ¼ç‡:{len(pledge_stat_cache)} è´¨æŠ¼æ¬¡æ•°:{sum(pledge_detail_cache.values())} å‡æŒ:{sum(holder_trade_cache.values())}")

def evaluate_risk_factors(ts_code: str) -> float:
    penalty = 0

    pledg_ratio = pledge_stat_cache.get(ts_code, 0)
    if pledg_ratio >= 60:
        penalty -= 12
    elif pledg_ratio >= 40:
        penalty -= 8
    elif pledg_ratio >= 30:
        penalty -= 5

    pledge_times = pledge_detail_cache.get(ts_code, 0)
    if pledge_times >= 5:
        penalty -= 4
    elif pledge_times >= 2:
        penalty -= 2

    reduce_times = holder_trade_cache.get(ts_code, 0)
    if reduce_times >= 3:
        penalty -= 6
    elif reduce_times >= 1:
        penalty -= 3

    if penalty != 0:
        logger.debug(f"{ts_code} é£é™©æ‰£åˆ†ï¼š{penalty} (è´¨æŠ¼ç‡: {pledg_ratio}%, è´¨æŠ¼æ¬¡æ•°: {pledge_times}, å‡æŒæ¬¡æ•°: {reduce_times})")
    return penalty


    
# ===== æ‰¹é‡ç¼“å­˜æ•°æ® =====
industry_cache = {}          # è¡Œä¸šä¿¡æ¯ç¼“å­˜ï¼šts_code -> è¡Œä¸šåç§°
concept_cache = {}           # æ¦‚å¿µé¢˜æç¼“å­˜ï¼šts_code -> æ¦‚å¿µåˆ—è¡¨
moneyflow_scores = {}        # èµ„é‡‘æµå‘å¾—åˆ†ç¼“å­˜ï¼šts_code -> å¾—åˆ†
concept_list_cache = {}      # æ¦‚å¿µåç§°ä¸IDæ˜ å°„ç¼“å­˜ï¼šconcept_name -> concept_id
concept_detail_cache = {}    # æ¦‚å¿µæ¶¨å¹…ç¼“å­˜ï¼šconcept_id -> å¹³å‡æ¶¨å¹…
pledge_stat_cache = {}       # è´¨æŠ¼ç‡ç¼“å­˜ï¼šts_code -> è´¨æŠ¼ç‡(%)
pledge_detail_cache = {}     # è´¨æŠ¼æ¬¡æ•°ç¼“å­˜ï¼šts_code -> æ¬¡æ•°
holder_trade_cache = {}      # è‚¡ä¸œå‡æŒæ¬¡æ•°ç¼“å­˜ï¼šts_code -> æ¬¡æ•°
block_trade_cache = {}       # å¤§å®—äº¤æ˜“æ¬¡æ•°ç¼“å­˜ï¼šts_code -> æ¬¡æ•°
stk_limit_cache = {}         # æ¶¨è·Œåœä»·ç¼“å­˜ï¼šts_code -> {'up_limit': x, 'down_limit': y}
share_float_cache = {}      # é™å”®è§£ç¦ç¼“å­˜
holdernumber_cache = {}     # è‚¡ä¸œäººæ•°ç¼“å­˜
express_cache = {}          # ä¸šç»©å¿«æŠ¥ç¼“å­˜

# ===== å·¥å…·å‡½æ•° =====
def get_strategy_type(strategy_name: str) -> str:
    for group_name, strategy_list in STRATEGY_GROUPS.items():
        if strategy_name in strategy_list:
            return group_name
    return "æœªçŸ¥"

# ===== è·å–è¡Œä¸šä¸é¢˜æä¿¡æ¯ =====
def get_industry(ts_code: str) -> str:
    return industry_cache.get(ts_code, "æœªçŸ¥è¡Œä¸š")

def get_concepts(ts_code: str) -> List[str]:
    return concept_cache.get(ts_code, [])

# ===== åˆ†æ•£åº¦ç®—æ³• =====
def diversify_recommendations(scored_stocks: List[Tuple], max_recommend=10, min_score_threshold=0) -> List[Tuple]:
    # â­ å¾—åˆ†ä»é«˜åˆ°ä½æ’åº
    scored_stocks = sorted(scored_stocks, key=lambda x: x[0], reverse=True)

    industry_count = {}
    concept_count = {}
    final_selection = []
    dynamic_industry_limit = 3
    dynamic_concept_limit = 2

    for stock in scored_stocks:
        score, ts_code, name, matched, pct_change, df, score_details = stock

        if score < min_score_threshold:
            continue

        # å‰ä¸‰åä¸å—è¡Œä¸šå’Œé¢˜æé™åˆ¶ï¼Œå¿…é¡»æ”¾åœ¨æœ€å‰é¢
        if len(final_selection) < 3:
            final_selection.append(stock)
            industry = get_industry(ts_code)
            concepts = get_concepts(ts_code)
            industry_count[industry] = industry_count.get(industry, 0) + 1
            for c in concepts:
                concept_count[c] = concept_count.get(c, 0) + 1
            continue

        industry = get_industry(ts_code)
        concepts = get_concepts(ts_code)

        if len(final_selection) >= max_recommend:
            break

        remaining_slots = max_recommend - len(final_selection)
        if remaining_slots <= 3:
            dynamic_industry_limit = 4
            dynamic_concept_limit = 3

        # è¡Œä¸šé™åˆ¶
        if industry and industry_count.get(industry, 0) >= dynamic_industry_limit:
            continue

        # é¢˜æé™åˆ¶
        if concepts and any(concept_count.get(c, 0) >= dynamic_concept_limit for c in concepts):
            continue

        final_selection.append(stock)
        industry_count[industry] = industry_count.get(industry, 0) + 1
        for c in concepts:
            concept_count[c] = concept_count.get(c, 0) + 1

    # âœ… æœ€åä¿é™©å†æ’ä¸€æ¬¡å¾—åˆ†
    final_selection = sorted(final_selection, key=lambda x: x[0], reverse=True)

    return final_selection





# ===== æ ¹æ®å¸‚åœºè¡Œæƒ…åŠ¨æ€è°ƒæ•´ç­–ç•¥æƒé‡ =====

def adjust_strategy_weights_by_market(trade_date: str = None) -> Dict[str, float]:
    """æ ¹æ®å¸‚åœºè¡Œæƒ…åŠ¨æ€è°ƒæ•´ç­–ç•¥æƒé‡ï¼ˆæ”¯æŒå›æµ‹æ¨¡å¼ï¼‰"""
    if IS_BACKTEST and CURRENT_TRADE_DATE:
        trade_date = CURRENT_TRADE_DATE  # å¼ºåˆ¶ä½¿ç”¨å›æµ‹æ—¥æœŸ
    try:
        # ===== æ—¥æœŸå¤„ç†å¢å¼ºç‰ˆ =====
        current_trade_date_str = get_valid_trade_date(
            api_func=pro.daily,  # æ˜ç¡®æŒ‡å®šæ¥å£å‡½æ•°
            date_field='trade_date',  # æ˜ç¡®æ—¥æœŸå­—æ®µåç§°
            base_date=datetime.strptime(trade_date, "%Y%m%d") if trade_date else datetime.today(),
            max_back_days=5
        )
        
        if not current_trade_date_str:
            logger.error("âŒ æ— æ³•è·å–æœ‰æ•ˆäº¤æ˜“æ—¥")
            return STRATEGY_TYPE_WEIGHTS.copy()
            
        # å°†å­—ç¬¦ä¸²è½¬æ¢ä¸º datetime å¯¹è±¡
        current_trade_date = datetime.strptime(current_trade_date_str, "%Y%m%d")
        logger.info(f"ğŸ“† æœ€ç»ˆå¤„ç†æ—¥æœŸï¼š{current_trade_date.strftime('%Y%m%d')}")

        # ===== å¸‚åœºæŒ‡æ ‡è®¡ç®— =====
        market_data = _get_market_indicators(current_trade_date)
        if market_data is None:
            return STRATEGY_TYPE_WEIGHTS.copy()

        pct_change, volatility, market_momentum, market_status, ma20 = market_data
        
        # ===== å¸‚åœºç»“æœæ—¥å¿— =====
        logger.info(f"å½“å‰å¸‚åœºçŠ¶æ€ï¼š{market_status} (æ¶¨è·Œå¹…: {pct_change:.2%}, æ³¢åŠ¨ç‡: {volatility:.2%}, åŠ¨é‡: {market_momentum:.2%}, 20æ—¥å‡çº¿: {ma20:.2f})")
        
        # ===== åŠ¨æ€æƒé‡è°ƒæ•´æ ¸å¿ƒé€»è¾‘ =====
        adjusted = STRATEGY_TYPE_WEIGHTS.copy()
        
        # æ ¹æ®å¸‚åœºçŠ¶æ€è°ƒæ•´
        adjustment_rules = {
            "æç«¯ç†Šå¸‚": {
                "è¶‹åŠ¿å‹": 0.9,
                "åè½¬å‹": 1.2,
                "å¸‚åœºä¸­æ€§å‹": 1.2,
                "é£é™©å‹": 1.3
            },
            "ç†Šå¸‚": {
                "è¶‹åŠ¿å‹": 0.95,
                "åŠ¨é‡å‹": 0.9,
                "åè½¬å‹": 1.1,
                "å¸‚åœºä¸­æ€§å‹": 1.1,
                "é£é™©å‹": 1.1
            },
            "æ¸©å’Œç†Šå¸‚": {
                "è¶‹åŠ¿å‹": 1.0,
                "åè½¬å‹": 1.05,
                "å¸‚åœºä¸­æ€§å‹": 1.0,
                "é£é™©å‹": 1.0
            },
            "éœ‡è¡å¸‚": {
                "è¶‹åŠ¿å‹": 1.05,  
                "åŠ¨é‡å‹": 1.05,  
                "å¸‚åœºä¸­æ€§å‹": 1.0,  
                "åè½¬å‹": 1.05,  
                "é£é™©å‹": 1.0
            },
            "æ¸©å’Œç‰›å¸‚": {
                "è¶‹åŠ¿å‹": 1.1,
                "åŠ¨é‡å‹": 1.15
            },
            "ç‰›å¸‚": {
                "è¶‹åŠ¿å‹": 1.15,
                "åŠ¨é‡å‹": 1.2
            },
            "æç«¯ç‰›å¸‚": {
                "è¶‹åŠ¿å‹": 1.2,
                "åŠ¨é‡å‹": 1.25
            }
        }
        
        # åº”ç”¨åŸºç¡€è°ƒæ•´è§„åˆ™
        if market_status in adjustment_rules:
            for key, factor in adjustment_rules[market_status].items():
                adjusted[key] *= factor
                
        # æ³¢åŠ¨ç‡åŠ¨æ€è°ƒæ•´ï¼ˆçº¿æ€§æ’å€¼ï¼‰
        volatility_factor = np.interp(volatility, [0.10, 0.40], [0.7, 1.3])
        adjusted.update({
            "å¸‚åœºä¸­æ€§å‹": min(adjusted.get("å¸‚åœºä¸­æ€§å‹", 1.0) * volatility_factor, 2.0),
            "é£é™©å‹": np.clip(adjusted.get("é£é™©å‹", -1.0) * (1.5 - 0.7 * volatility_factor), -2.0, 0.0)
        })
        
        # åŠ¨é‡è¡¥å¿è°ƒæ•´
        momentum_bonus = market_momentum * max(0.5 - 0.2 * abs(market_momentum), 0.3)
        adjusted["åŠ¨é‡å‹"] = np.clip(adjusted["åŠ¨é‡å‹"] + momentum_bonus, 0.6, 2.5)
        adjusted["è¶‹åŠ¿å‹"] = np.clip(adjusted["è¶‹åŠ¿å‹"] + momentum_bonus * 0.7, 0.5, 2.0)
        
        risk_weight = adjusted["é£é™©å‹"]
        if market_status in ["ç‰›å¸‚", "æç«¯ç‰›å¸‚"]:
            risk_weight = max(risk_weight, -1.5)  # ç‰›å¸‚ä¸­æƒ©ç½šä¸Šé™-1.5
        elif market_status == "æç«¯ç†Šå¸‚":
            risk_weight = max(risk_weight, -3.0)  # æç«¯ç†Šå¸‚å…è®¸-3.0
        adjusted["é£é™©å‹"] = risk_weight
        
        # é£é™©æƒé‡è¾¹ç•Œæ§åˆ¶
        weight_limits = {
            "è¶‹åŠ¿å‹": (0.5, 1.5),
            "åŠ¨é‡å‹": (0.6, 1.6),
            "åè½¬å‹": (0.7, 1.8),
            "å¸‚åœºä¸­æ€§å‹": (0.8, 2.0),
            "é£é™©å‹": (-3.0, 0.0)
        }
        
        # åº”ç”¨è¾¹ç•Œé™åˆ¶
        for strategy_type in adjusted:
            if strategy_type in weight_limits:
                min_val, max_val = weight_limits[strategy_type]
                adjusted[strategy_type] = np.clip(adjusted[strategy_type], min_val, max_val)
        
        
        return adjusted

    except Exception as e:
        logger.error(f"âŒ æƒé‡è°ƒæ•´è¿‡ç¨‹å¼‚å¸¸ï¼š{str(e)}\n{traceback.format_exc()}")
        return STRATEGY_TYPE_WEIGHTS.copy()




def _get_market_indicators(trade_date: datetime) -> Optional[Tuple[float, float, float, str, float]]:
    """è·å–ä¸‰å¤§å¸‚åœºæŒ‡æ ‡ï¼šæ¶¨è·Œå¹…ã€æ³¢åŠ¨ç‡ã€åŠ¨é‡ï¼Œå¹¶è®¡ç®—20æ—¥å‡çº¿ï¼ˆMA20ï¼‰åŠåˆ¤æ–­å¸‚åœºçŠ¶æ€"""
    try:
        # è·å–æ²ªæ·±300æ•°æ®
        hs300 = safe_api_call(
            pro.index_daily,
            ts_code="000300.SH",
            start_date=(trade_date - timedelta(days=60)).strftime('%Y%m%d'),
            end_date=trade_date.strftime('%Y%m%d'),
            fields="trade_date,close"
        )
        
        if len(hs300) < 20:
            logger.warning("âš ï¸ æ•°æ®ä¸è¶³20ä¸ªäº¤æ˜“æ—¥")
            return None
        
        # è®¡ç®—å¸‚åœºæŒ‡æ ‡
        closes = hs300['close'].astype(float)
        returns = closes.pct_change().dropna()
        
        # è®¡ç®—æ¶¨è·Œå¹…
        pct_change = (closes.iloc[-1] - closes.iloc[0]) / closes.iloc[0]
        
        # è®¡ç®—æ³¢åŠ¨ç‡ï¼ˆå¹´åŒ–ï¼‰
        volatility = returns.std() * np.sqrt(252)
        
        # è®¡ç®—åŠ¨é‡
        momentum = closes[-20:].mean() / closes[:-20].mean() - 1
        
        # è®¡ç®—20æ—¥å‡çº¿ï¼ˆMA20ï¼‰
        ma20 = closes.rolling(20).mean().iloc[-1]
        
        # è·å–å¸‚åœºçŠ¶æ€
        market_status = _determine_market_status(pct_change, volatility, ma20, closes.iloc[-1], momentum)
        
        return pct_change, volatility, momentum, market_status, ma20
        
    except Exception as e:
        logger.error(f"âŒ è·å–å¸‚åœºæŒ‡æ ‡å¤±è´¥ï¼š{str(e)}")
        return None


def _determine_market_status(pct_change: float, volatility: float, ma20: float, last_close: float, momentum: float) -> str:
    """ç»¼åˆåˆ¤æ–­å¸‚åœºçŠ¶æ€"""
    # é¦–å…ˆæ ¹æ®æ³¢åŠ¨ç‡åˆ¤æ–­æ˜¯å¦ä¸ºé«˜æ³¢åŠ¨éœ‡è¡å¸‚
    if volatility > 0.25:
        return "é«˜æ³¢åŠ¨éœ‡è¡å¸‚"
    
    # æ ¹æ®æ¶¨è·Œå¹…å’Œ20æ—¥å‡çº¿å…±åŒåˆ¤æ–­å¸‚åœºçŠ¶æ€
    if pct_change < -0.15:
        return "æç«¯ç†Šå¸‚"
    elif pct_change < -0.05:
        return "ç†Šå¸‚"
    elif pct_change > 0.15:
        # åŠ¨é‡æ­£ä¸”è¾ƒé«˜ï¼Œæç«¯ç‰›å¸‚
        if momentum > 0.1 and last_close > ma20 * 1.08:
            return "æç«¯ç‰›å¸‚"
        # åŠ¨é‡æ­£ä½†è¾ƒä½ï¼Œç‰›å¸‚
        elif momentum > 0.05:
            return "ç‰›å¸‚"
        # æ¶¨å¹…å¤§äº15%ï¼Œä½†æ”¶ç›˜ä»·ä¸å¤§äºMA20çš„1.08å€ï¼Œç‰›å¸‚
        else:
            return "ç‰›å¸‚"
    elif pct_change > 0.05:
        # åŠ¨é‡æ­£ä¸”è¾ƒé«˜ï¼Œç‰›å¸‚
        if momentum > 0.05 and last_close > ma20 * 1.08:
            return "ç‰›å¸‚"
        # åŠ¨é‡è´Ÿä¸”è¾ƒä½ï¼Œç†Šå¸‚
        elif momentum < -0.05 or last_close < ma20 * 0.92:
            return "ç†Šå¸‚"
    
    # é»˜è®¤éœ‡è¡å¸‚
    return "éœ‡è¡å¸‚"





def get_valid_trade_date(
        api_func, 
        date_field: str, 
        base_date: Optional[datetime] = None, 
        max_back_days: int = 5, 
        **api_kwargs
    ) -> Optional[str]:
    """
    ä¿®å¤ç‚¹1ï¼šç¡®ä¿è¿”å›ç»Ÿä¸€æ ¼å¼çš„æ—¥æœŸå­—ç¬¦ä¸²
    æ–°å¢æœ‰æ•ˆæ€§æ£€æŸ¥é€»è¾‘
    """
    if appy.IS_BACKTEST:
        return base_date.strftime('%Y%m%d') 
    base_date = base_date or datetime.today()
    for delta in range(max_back_days + 1):
        current_date = base_date - timedelta(days=delta)
        d = current_date.strftime('%Y%m%d')
        df = safe_api_call(api_func, **api_kwargs, **{date_field: d})
        
        # æ–°å¢æ•°æ®æœ‰æ•ˆæ€§æ£€æŸ¥
        if not df.empty and 'ts_code' in df.columns and len(df['ts_code'].unique()) > 10:
            logger.info(f"âœ… éªŒè¯æœ‰æ•ˆäº¤æ˜“æ—¥: {d} | åŒ…å«è‚¡ç¥¨æ•°: {len(df)}")
            return d
    logger.error(f"âŒ æœªæ‰¾åˆ°æœ‰æ•ˆäº¤æ˜“æ—¥ï¼ˆæœ€è¿‘ {max_back_days} å¤©ï¼‰")
    return None


# ===== å…¨å±€å®šä¹‰ï¼šQueryç±»æ¥å£åå• =====
QUERY_INTERFACE_NAMES = [
    # åŸºç¡€è¡Œæƒ…ç±»
    'daily', 'daily_basic', 'moneyflow', 
    # è´¢åŠ¡æŒ‡æ ‡ç±»
    'fina_indicator_vip', 'express', 'forecast',
    # å¸‚åœºå‚è€ƒç±»
    'stk_limit', 'limit_list_d', 'suspend_d', 'block_trade',
    # è‚¡ä¸œè‚¡æƒç±»
    'stk_holdernumber', 'stk_holdertrade', 'pledge_stat', 'pledge_detail',
    # åŸºç¡€ä¿¡æ¯ç±» 
    'stock_basic', 'concept_detail', 'index_weight', 'index_member',
    # æŒ‡æ•°æ•°æ®ç±»
    'index_daily',
    # ç‰¹è‰²æ•°æ®ç±»
    'share_float', 'anns_d',
    # åŒèŠ±é¡ºæ¦‚å¿µæ¿å—æ•°æ®ç±»ï¼ˆæ–°æ·»åŠ çš„æ¥å£ï¼‰
    'ths_index'  # åŒèŠ±é¡ºæ¦‚å¿µæ¿å—æ¥å£
]

def safe_api_call(func, *args, retries=3, delay=2, **kwargs):
    """ å°è£…æ¥å£è°ƒç”¨ï¼Œè‡ªåŠ¨é€‰æ‹©é™é€Ÿå™¨ï¼Œå¤„ç†å¼‚å¸¸ï¼Œä¿è¯è¿”å› DataFrame """
    
    # å¦‚æœ func æ˜¯ functools.partial å¯¹è±¡ï¼Œè·å–å…¶åŸå§‹å‡½æ•°
    if isinstance(func, functools.partial):
        api_name = func.args[0] if func.args else kwargs.get('api_name')  # è·å– api_name
    else:
        api_name = func.__name__  # è·å–å‡½æ•°åç§°ä½œä¸º API å
    
    # ç¡®ä¿ api_name ä¸ API_CONFIG ä¸­çš„é”®ä¸€è‡´
    if api_name not in API_CONFIG:
        api_name = kwargs.get('api_name', api_name)  # å¦‚æœ API_CONFIG ä¸­æ²¡æœ‰è¯¥æ¥å£ï¼Œä½¿ç”¨ kwargs ä¸­ä¼ é€’çš„ api_name
    
    # è¯¦ç»†æ—¥å¿—è®°å½•
    logger.debug(f"å‡†å¤‡è°ƒç”¨æ¥å£: {api_name} å‚æ•°: {kwargs}")

    # é™é€Ÿå™¨
    if api_name in QUERY_INTERFACE_NAMES or 'query' in api_name.lower():
        query_rate_limiter.wait(interface_type="QUERYæ¥å£")
        logger.debug(f"âœ… å·²åº”ç”¨QUERYæ¥å£é™é€Ÿ: {api_name}")
    else:
        normal_rate_limiter.wait(interface_type="æ™®é€šæ¥å£")
        logger.debug(f"âœ… å·²åº”ç”¨æ™®é€šæ¥å£é™é€Ÿ: {api_name}")

    # è·å–APIé…ç½®ä¿¡æ¯
    api_config = API_CONFIG.get(api_name, {})  # è·å–æ¥å£é…ç½®ï¼Œé»˜è®¤ç©ºå­—å…¸
    fields = api_config.get('fields', [])       # è·å–å­—æ®µåˆ—è¡¨ï¼Œé»˜è®¤ç©ºåˆ—è¡¨
    
    # æ£€æŸ¥æ˜¯å¦éœ€è¦ä¸ºæ¥å£æ·»åŠ  trade_date å‚æ•°
    if 'trade_date' in fields:
        kwargs['trade_date'] = appy.CURRENT_TRADE_DATE
        logger.debug(f"âœ… ä¸º {api_name} æ·»åŠ  trade_date å‚æ•°: {appy.CURRENT_TRADE_DATE}")
    else:
        logger.debug(f"â© {api_name} æœªé…ç½® trade_date å­—æ®µ")
    
    # å°è¯•è°ƒç”¨æ¥å£ï¼Œæœ€å¤šé‡è¯• retries æ¬¡
    for attempt in range(1, retries + 1):
        try:
            result = func(*args, **kwargs)
            if isinstance(result, pd.DataFrame):
                logger.debug(f"âœ… æ¥å£è°ƒç”¨æˆåŠŸ: {api_name} è¿”å› {len(result)} è¡Œæ•°æ®")
                return result
            else:
                # è¿”å›ç±»å‹ä¸æ˜¯ DataFrame æ—¶å¤„ç†
                logger.warning(f"âš ï¸ {api_name} è¿”å›é DataFrame ç±»å‹: {type(result)}")
                # å¦‚æœè¿”å›ä¸æ˜¯ DataFrameï¼Œå¯ä»¥è¿”å›ç©º DataFrame æˆ–åšå…¶ä»–å¤„ç†
                return pd.DataFrame()  
        except Exception as e:
            error_msg = str(e)
            if "è¯·æŒ‡å®šæ­£ç¡®çš„æ¥å£å" in error_msg or "parameter" in error_msg.lower():
                logger.error(f"âŒ {api_name} å‚æ•°é”™è¯¯æˆ–æ¥å£åé”™è¯¯: {error_msg}")
                break
            logger.warning(f"âš ï¸ è°ƒç”¨ {api_name} å¤±è´¥ï¼ˆç¬¬{attempt}æ¬¡ï¼‰: {error_msg}")
            time.sleep(delay * attempt)  # æŒ‡æ•°é€€é¿

    logger.error(f"ğŸš« è°ƒç”¨ {api_name} å®Œå…¨å¤±è´¥ï¼Œè¿”å›ç©º DataFrame")
    return pd.DataFrame()  # å¦‚æœè°ƒç”¨å¤±è´¥ï¼Œè¿”å›ç©º DataFrame









# ===== é…ç½®æ—¥å¿—ç³»ç»Ÿ =====
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
    handlers=[logging.StreamHandler(sys.stdout)]
)
logger = logging.getLogger(__name__)

# ===== ç³»ç»Ÿé…ç½® =====
class Config:
    MAX_STOCKS_TO_ANALYZE = 5500         # ä¿æŒä¸å˜ï¼Œæœ€å¤§åˆ†æè‚¡ç¥¨æ•°é‡
    MIN_DATA_DAYS = 30

    
    POINTS = 10000
    MAX_CALLS_PER_MIN = 1000             # æ¯åˆ†é’Ÿ1000æ¬¡
    MAX_WORKERS = 24                 


# ===== åŠ¨æ€é™é€Ÿæ§åˆ¶å™¨ =====
class RateLimiter:
    def __init__(self, max_calls: int, period: int):
        self.max_calls = max_calls
        self.period = period
        self.lock = threading.Lock()
        self.calls = []

    def wait(self, interface_type=None) -> None:
        # æ™ºèƒ½åˆ¤æ–­æ¥å£ç±»å‹
        if interface_type is None:
            if self.max_calls == 1000:
                interface_type = "QUERYæ¥å£"
            elif self.max_calls == 1000:
                interface_type = "æ™®é€šæ¥å£"
            else:
                interface_type = "æ¥å£"

        with self.lock:
            now = time.time()
            self.calls = [call for call in self.calls if now - call < self.period]

            if len(self.calls) >= self.max_calls:
                sleep_time = self.period - (now - self.calls[0])
                sleep_time = max(sleep_time, 0)
                if sleep_time > 0:
                    logger.info(f"â³ [{interface_type}] é™é€Ÿä¸­ï¼šç­‰å¾… {sleep_time:.1f}s  (é™é¢ {self.max_calls}/{self.period}s)")
                    time.sleep(sleep_time)

            self.calls.append(time.time())


# åˆå§‹åŒ–ä¸¤ä¸ªé™é€Ÿå™¨
normal_rate_limiter = RateLimiter(1000, 60)
query_rate_limiter = RateLimiter(1000, 60)





# ===== API é…ç½® =====


# éªŒè¯é…ç½®
AppConfig.validate()
AppConfig.create_dirs()

# åˆå§‹åŒ–API
ts.set_token(AppConfig.TUSHARE_TOKEN)
pro = ts.pro_api()
DEEPSEEK_API_KEY = AppConfig.DEEPSEEK_API_KEY
# ===== DeepSeek API äº¤äº’ç±» =====
class DeepSeekAPI:
    @staticmethod
    def call_deepseek(prompt: str) -> str:
        """è°ƒç”¨DeepSeek APIè·å–ç­–ç•¥åˆ†æ"""
        try:
            headers = {
                "Authorization": f"Bearer {DEEPSEEK_API_KEY}",
                "Content-Type": "application/json"
            }
            payload = {
                "model": "deepseek-chat",
                "messages": [
                    {
                        "role": "system",
                        "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„è‚¡ç¥¨åˆ†æAIåŠ©æ‰‹ã€‚è¯·ä»ç”¨æˆ·è¾“å…¥ä¸­è¯†åˆ«è‚¡ç¥¨æŠ€æœ¯åˆ†æç­–ç•¥ï¼Œ"
                                  "åªè¿”å›JSONæ ¼å¼çš„ç­–ç•¥åˆ—è¡¨å’Œè§£é‡Šã€‚"
                    },
                    {
                        "role": "user",
                        "content": f"ä»ä»¥ä¸‹æ–‡æœ¬ä¸­è¯†åˆ«è‚¡ç¥¨æŠ€æœ¯åˆ†æç­–ç•¥: {prompt}"
                    }
                ],
                "temperature": 0.3
            }
            
            response = requests.post(
                "https://api.deepseek.com/v1/chat/completions",
                headers=headers,
                json=payload,
                timeout=30
            )
            response.raise_for_status()
            return response.json()["choices"][0]["message"]["content"]
        except Exception as e:
            logger.error(f"è°ƒç”¨DeepSeek APIå¤±è´¥: {str(e)}")
            return ""

    @staticmethod
    def parse_strategies(response: str) -> Tuple[List[str], str, Dict[str, int]]:
        """è§£æDeepSeek APIè¿”å›çš„ç­–ç•¥"""
        try:
            # ç®€å•å®ç° - å®é™…åº”æ ¹æ®APIè¿”å›æ ¼å¼è°ƒæ•´
            if not response:
                return [], "æ— æ³•è§£æç­–ç•¥", {}
                
            # è¿™é‡Œå‡è®¾APIè¿”å›çš„æ˜¯ç­–ç•¥åˆ—è¡¨
            # å®é™…å®ç°éœ€è¦æ ¹æ®APIå®é™…è¿”å›æ ¼å¼è°ƒæ•´
            strategies = []
            explanation = "è¯†åˆ«åˆ°çš„ç­–ç•¥:\n"
            custom_weights = {}
            
            for strategy in STRATEGY_WEIGHTS.keys():
                if strategy in response:
                    strategies.append(strategy)
                    explanation += f"- {strategy}\n"
                    custom_weights[strategy] = STRATEGY_WEIGHTS[strategy]
            
            return strategies, explanation, custom_weights
        except Exception as e:
            logger.error(f"è§£æç­–ç•¥å¤±è´¥: {str(e)}")
            return [], "è§£æç­–ç•¥æ—¶å‡ºé”™", {}
# ===== è‚¡ç¥¨åˆ†ææ ¸å¿ƒç±» =====
class StockAnalyzer:
    pro = ts.pro_api(AppConfig.TUSHARE_TOKEN) 
    @staticmethod
    def get_valid_daily_data(
        api_func: Any,
        fields: str,
        max_days_back: int = 10,
        extra_kwargs: Optional[Dict[str, Any]] = None,
        base_date: Optional[datetime] = None   # â­ æ–°å¢å‚æ•°
    ) -> Tuple[pd.DataFrame, Optional[str]]:
        extra_kwargs = extra_kwargs or {}
        func_name = getattr(api_func, '__name__', None)
        if func_name is None and hasattr(api_func, 'func'):
            func_name = getattr(api_func.func, '__name__', str(api_func))
        if func_name is None:
            func_name = str(api_func)

        valid_date = get_valid_trade_date(
            api_func,
            date_field='trade_date',
            base_date=base_date or datetime.today(),
            max_back_days=max_days_back,
            fields=fields,
            **extra_kwargs
        )
        if valid_date is None:
            logger.error(f"âŒ è¿ç»­ {max_days_back} å¤©æ— æœ‰æ•ˆæ•°æ® ({func_name})")
            return pd.DataFrame(), None

        df = safe_api_call(
            api_func,
            trade_date=valid_date,
            fields=fields,
            **extra_kwargs
        )
        if df.empty:
            logger.error(f"âŒ æ‰¾åˆ°äº¤æ˜“æ—¥ {valid_date}ï¼Œä½†æ‹‰å–æ•°æ®ä»ä¸ºç©º ({func_name})")
            return pd.DataFrame(), None

        logger.info(f"âœ… æˆåŠŸè·å– {valid_date} çš„æ•°æ® ({func_name})")
        return df, valid_date

    @staticmethod
    def get_single_stock_info(ts_code: str) -> Optional[Dict]:
        try:
            if not ts_code.endswith(('.SH', '.SZ')):
                ts_code = f"{ts_code}.SZ" if ts_code.startswith(('00', '30')) else f"{ts_code}.SH"
            basic = safe_api_call(pro.stock_basic, ts_code=ts_code, fields='ts_code,name,industry,list_date,market')
            daily = safe_api_call(pro.daily, ts_code=ts_code,
                                  start_date=(datetime.now() - timedelta(days=30)).strftime('%Y%m%d'),
                                  end_date=datetime.now().strftime('%Y%m%d'))
            if basic.empty or daily.empty:
                return None
            daily = daily.sort_values('trade_date')
            daily.index = pd.to_datetime(daily['trade_date'])

            indicators, _ = StockAnalyzer.calculate_technical_indicators(daily)
            latest_signals = {strategy: bool(values.iloc[-1]) for strategy, values in indicators.items()}

            return {
                'basic_info': basic.iloc[0].to_dict(),
                'price_info': daily.iloc[-1].to_dict(),
                'technical_signals': latest_signals
            }
        except Exception as e:
            logger.error(f"æŸ¥è¯¢è‚¡ç¥¨ {ts_code} å¤±è´¥: {str(e)}")
            return None

    @staticmethod
    def get_stock_list(
        selected_markets: Tuple[str, ...], 
        max_count: Optional[int] = None, 
        strategy_mode: str = "é»˜è®¤", 
        trade_date: Optional[str] = None
    ) -> Tuple[List[Tuple[str, str]], Dict[str, float]]:
        """ä¼˜åŒ–ç‚¹ï¼š
        1. æ‰¹é‡è·å–æ‰€æœ‰æŒ‡æ•°æˆåˆ†è‚¡ï¼ˆå‡å°‘APIè°ƒç”¨æ¬¡æ•°ï¼‰
        2. åˆå¹¶è¡Œæƒ…æ•°æ®æŸ¥è¯¢ï¼ˆåŸ2æ¬¡â†’1æ¬¡ï¼‰
        3. ä¼˜åŒ–å¸‚åœºç­›é€‰é€»è¾‘ï¼ˆå‡å°‘å¾ªç¯æ¬¡æ•°ï¼‰
        4. æ·»åŠ LRUç¼“å­˜è£…é¥°å™¨
        """
        from market_utils import get_market_status
        try:
            # 1. è·å–å…¨é‡è‚¡ç¥¨æ•°æ®ï¼ˆå•æ¬¡APIè°ƒç”¨ï¼‰
            df = safe_api_call(
                pro.stock_basic,
                exchange='',
                list_status='L',
                fields='ts_code,name,market,list_date,industry'
            )
            if df.empty:
                logger.error("âŒ è·å–è‚¡ç¥¨åŸºç¡€æ•°æ®å¤±è´¥")
                return [], {}

            # 2. è·å–åˆå¹¶åçš„è¡Œæƒ…æ•°æ®ï¼ˆå•æ¬¡APIè°ƒç”¨ï¼‰
            base_date = datetime.strptime(trade_date, '%Y%m%d') if trade_date else None
            valid_date = get_valid_trade_date(
                pro.daily,
                date_field='trade_date',
                base_date=base_date,
                max_back_days=5
            )
            if not valid_date:
                logger.error("âŒ æ— æ³•è·å–æœ‰æ•ˆäº¤æ˜“æ—¥")
                return [], {}

            # æ‰¹é‡è·å–æ‰€æœ‰è¡Œæƒ…æ•°æ®
            daily_data = safe_api_call(
                pro.daily,
                trade_date=valid_date,
                fields='ts_code,open,close,high,low,pct_chg,vol,amount'  # ç¡®ä¿åŒ…å«openå­—æ®µç”¨äºåˆ¤æ–­é˜³çº¿
            )
            daily_basic = safe_api_call(
                pro.daily_basic,
                trade_date=valid_date,
                fields='ts_code,total_mv,turnover_rate'
            )

            # 3. åˆå¹¶æ•°æ®
            df = df.merge(daily_basic, on='ts_code', how='left').merge(
                daily_data, on='ts_code', how='left'
            )
            total_before_filter = len(df)

            # è·å–å½“å¤©åœç‰Œè‚¡ç¥¨åˆ—è¡¨
            suspend_df = pro.suspend_d(suspend_type='S', trade_date=valid_date)
            suspend_stocks = set(suspend_df['ts_code'])  # åœç‰Œè‚¡ç¥¨çš„ TS ä»£ç é›†åˆ

            # ç¡®ä¿åœç‰Œè‚¡ç¥¨ä¿¡æ¯æ­£ç¡®
            logger.info(f"åœç‰Œè‚¡ç¥¨æ•°ï¼š{len(suspend_stocks)}")  # è¾“å‡ºåœç‰Œè‚¡ç¥¨çš„æ•°é‡ï¼Œä¾¿äºè°ƒè¯•
            logger.info(f"åœç‰Œè‚¡ç¥¨ç¤ºä¾‹ï¼š{list(suspend_stocks)[:5]}")  # è¾“å‡ºä¸€äº›åœç‰Œè‚¡ç¥¨ç¤ºä¾‹ï¼ŒæŸ¥çœ‹æ•°æ®æ˜¯å¦æ­£ç¡®

            # 4. ç­›é™¤åœç‰Œè‚¡ç¥¨
            df = df[~df['ts_code'].isin(suspend_stocks)]
            logger.info(f"ğŸš« ç­›é™¤åœç‰Œè‚¡ï¼š{total_before_filter} âœ {len(df)}")

            # 5. å¤„ç†å¸‚åœº/æ¿å—é€‰æ‹©
            index_map = {
                "ä¸­è¯500": "000905.SH", "æ²ªæ·±300": "000300.SH", "ä¸Šè¯50": "000016.SH",
                "ä¸­è¯ç™½é…’": "399997.SZ", "ä¸­è¯æ¶ˆè´¹": "000932.SH", "ç§‘åˆ›50": "000688.SH",
                "æ·±è¯100": "399330.SZ", "åŒ—è¯50": "899050.BJ", "å›½è¯ETF": "399380.SZ"
            }

            # æ‰¹é‡è·å–æ‰€æœ‰éœ€è¦çš„æŒ‡æ•°æˆåˆ†è‚¡
            needed_indexes = [index_map[m] for m in selected_markets if m in index_map]
            index_members = {}  # å¦‚æœä¸éœ€è¦ `_get_index_members`ï¼Œç›´æ¥ä½¿ç”¨è¿™ä¸ªç©ºå­—å…¸

            # æ„å»ºç­›é€‰æ¡ä»¶
            market_filters = []
            hs300_set = set()

            for m in selected_markets:
                if m in ["ä¸»æ¿", "åˆ›ä¸šæ¿", "ç§‘åˆ›æ¿"]:
                    market_filters.append(df['market'] == m)
                elif m in index_map:
                    codes = index_members.get(index_map[m], set())
                    if m == "æ²ªæ·±300":
                        hs300_set = codes
                    market_filters.append(df['ts_code'].isin(codes))

            if market_filters:
                df = df[np.logical_or.reduce(market_filters)]
            else:
                logger.warning("âš ï¸ æœªåº”ç”¨ä»»ä½•å¸‚åœºç­›é€‰æ¡ä»¶")

            # 6. æ•°å€¼è½¬æ¢
            for col in ['close', 'open', 'high', 'low', 'total_mv', 'turnover_rate', 'pct_chg']:
                if col not in df.columns:
                    logger.warning(f"âš ï¸ åˆ— {col} ä¸å­˜åœ¨ï¼Œè·³è¿‡å¤„ç†è¯¥åˆ—")
                    continue
                df[col] = pd.to_numeric(df[col], errors='coerce')

            # 7. ç­›é€‰æ¡ä»¶é…ç½®
            mode_config = {
                "æ¿€è¿›å‹": {"mv": (100_000, 3_000_000), "turnover": (1, 35), "pct": (-6, 12), "pe": (15, 45), "pb": (0.8, 8), "roe": 6},
                "ç¨³å¥å‹": {"mv": (300_000, 6_000_000), "turnover": (0.5, 20), "pct": (-4, 8), "pe": (8, 35), "pb": (1.0, 5), "roe": 10},
                "ç©¿çº¿å‹": {"mv": (50_000, 10_000_000), "turnover": (0.2, 50), "pct": (-20, 30), "pe": (0, 200), "pb": (0.1, 20), "roe": 0},  # æ–°å¢ç©¿çº¿å‹ç­–ç•¥å‚æ•°
                "é»˜è®¤":   {"mv": (300_000, 3_000_000), "turnover": (1.5, 25), "pct": (-9, 28), "pe": (5, 50), "pb": (0.8, 8), "roe": 8}
            }
            cfg = mode_config.get(strategy_mode, mode_config["é»˜è®¤"])

            # 8. æ–°å¢è·å–åŸºæœ¬é¢æ•°æ®å¹¶è¿›è¡Œç­›é€‰
            fundamental_basic = safe_api_call(
                pro.daily_basic,
                trade_date=valid_date,
                fields='ts_code,pe_ttm,pb'
            )

            # è·å– fina_indicator_vip ä¸­çš„å‡€èµ„äº§æ”¶ç›Šç‡ï¼ˆroeï¼‰
            fundamental_fina = safe_api_call(
                pro.fina_indicator_vip,
                trade_date=valid_date,
                fields='ts_code,roe'
            )

            # åˆå¹¶æ•°æ®ï¼Œä½¿ç”¨ ts_code ä½œä¸ºè¿æ¥é”®
            df = df.merge(fundamental_basic, on='ts_code', how='left')
            df = df.merge(fundamental_fina, on='ts_code', how='left')

            # 9. æ·»åŠ åŸºæœ¬é¢ç­›é€‰æ¡ä»¶ï¼šå¸‚ç›ˆç‡ã€å‡€èµ„äº§æ”¶ç›Šç‡ã€å‡€èµ„äº§æ”¶ç›Šç‡ç­‰
            filtered = df[
                (df['pe_ttm'].between(cfg['pe'][0], cfg['pe'][1])) &  # åŠ¨æ€å¸‚ç›ˆç‡èŒƒå›´
                (df['pb'].between(cfg['pb'][0], cfg['pb'][1])) &  # åŠ¨æ€å¸‚å‡€ç‡èŒƒå›´
                (df['roe'] >= cfg['roe'])  # åŠ¨æ€ROEæ¡ä»¶
            ]
            logger.info(f"ğŸ“Š åŸºæœ¬é¢è¿‡æ»¤ï¼š{len(filtered)}")

            # 10. æ·»åŠ ç©¿çº¿å‹ç‰¹æ®Šç­›é€‰ï¼ˆæ›´ä¸¥æ ¼ç‰ˆæœ¬ï¼‰
            if strategy_mode == "ç©¿çº¿å‹":
                # è®¡ç®—éœ€è¦çš„å†å²æ—¥æœŸ
                end_date = datetime.strptime(valid_date, '%Y%m%d')
                start_date = (end_date - timedelta(days=30)).strftime('%Y%m%d')  # è·å–30å¤©çš„æ•°æ®

                # åˆå§‹åŒ–
                not_overbought_stocks = []

                # åˆ†æ‰¹å¤„ç†è‚¡ç¥¨ï¼Œæ¯æ‰¹æœ€å¤š100æ”¯
                batch_size = 100
                stock_codes = filtered['ts_code'].unique()[:max_count if max_count else len(filtered)]  # é™åˆ¶æ•°é‡

                logger.info(f"ğŸ“Š ç©¿çº¿å‹ç­–ç•¥ï¼šå¼€å§‹ç­›é€‰ {len(stock_codes)} æ”¯è‚¡ç¥¨")

                # å­˜å‚¨æœ€ç»ˆè¿‡æ»¤ç»“æœ
                filtered_stocks = []

                for i in range(0, len(stock_codes), batch_size):
                    batch_codes = stock_codes[i:i + batch_size]
                    batch_str = ','.join(batch_codes)  # å¤šä¸ªè‚¡ç¥¨ç”¨é€—å·åˆ†éš”

                    try:
                        # ä½¿ç”¨ pro.daily è·å–æ‰¹é‡è‚¡ç¥¨çš„å†å²æ•°æ®
                        hist_daily = pro.daily(
                            ts_code=batch_str,
                            start_date=start_date,
                            end_date=valid_date
                        )

                        if hist_daily is not None and not hist_daily.empty:
                            # æŒ‰è‚¡ç¥¨ä»£ç åˆ†ç»„å¤„ç†
                            for ts_code in batch_codes:
                                stock_data = hist_daily[hist_daily['ts_code'] == ts_code].sort_values('trade_date')

                                if len(stock_data) >= 5:
                                    try:
                                        # è®¡ç®—ç´¯è®¡æ¶¨å¹…
                                        pct_3d = stock_data['pct_chg'].tail(3).sum()
                                        pct_5d = stock_data['pct_chg'].tail(5).sum()
                                        pct_10d = stock_data['pct_chg'].tail(10).sum() if len(stock_data) >= 10 else pct_5d
                                        pct_20d = stock_data['pct_chg'].tail(20).sum() if len(stock_data) >= 20 else pct_10d
                                        pct_30d = stock_data['pct_chg'].sum()

                                        # è®¡ç®—å•æ—¥æœ€å¤§æ¶¨å¹…
                                        max_single_day = stock_data['pct_chg'].tail(10).max() if len(stock_data) >= 10 else stock_data['pct_chg'].max()

                                        # ğŸ” æˆäº¤é¢é‡èƒ½è¿‡æ»¤ + æ¶¨åœæ—¥è±å…
                                        avg_amount = stock_data['amount'].tail(5).mean()
                                        curr_amount = stock_data['amount'].iloc[-1]
                                        pct_chg_today = stock_data['pct_chg'].iloc[-1]
                                        vol_pass = (curr_amount > 1.5 * avg_amount) or (pct_chg_today >= 9.8)  # æ¶¨åœæ—¥è±å…

                                        # æ›´ä¸¥æ ¼çš„æ¶¨å¹…åˆ¤æ–­æ¡ä»¶
                                        is_not_overbought = (
                                            pct_3d < 20 and
                                            pct_5d < 10 and
                                            pct_10d < 15 and
                                            pct_20d < 20 and
                                            pct_30d < 30 and
                                            max_single_day < 7
                                        )

                                        recent_pullback = stock_data['pct_chg'].tail(5).min() < -3

                                        if vol_pass and (is_not_overbought or (recent_pullback and pct_5d < 20)):
                                            not_overbought_stocks.append(ts_code)

                                        if len(not_overbought_stocks) <= 20:
                                            logger.debug(f"{ts_code}: 3d={pct_3d:.1f}%, 5d={pct_5d:.1f}%, "
                                                         f"10d={pct_10d:.1f}%, 20d={pct_20d:.1f}%, "
                                                         f"max_single={max_single_day:.1f}%, vol_pass={vol_pass}, "
                                                         f"pullback={recent_pullback}, selected={ts_code in not_overbought_stocks}")

                                    except Exception as e:
                                        logger.warning(f"å¤„ç†{ts_code}æ—¶å‡ºé”™: {e}")
                        else:
                            logger.debug(f"æ‰¹æ¬¡ {i // batch_size + 1} æ— æ•°æ®è¿”å›")

                    except Exception as e:
                        logger.warning(f"è·å–å†å²æ•°æ®å¤±è´¥ï¼ˆæ‰¹æ¬¡ {i // batch_size + 1}ï¼‰: {e}")
                        for ts_code in batch_codes:
                            stock_row = filtered[filtered['ts_code'] == ts_code]
                            if not stock_row.empty:
                                if stock_row.iloc[0]['pct_chg'] <= 5:
                                    not_overbought_stocks.append(ts_code)

                # è¿‡æ»¤å‡ºæ¶¨å¹…é€‚ä¸­çš„è‚¡ç¥¨
                filtered = filtered.copy()
                filtered['is_not_overbought'] = filtered['ts_code'].isin(not_overbought_stocks)
                filtered_before = len(filtered)
                filtered = filtered[filtered['is_not_overbought']]

                logger.info(f"ğŸ“ˆ æ¶¨å¹…ç­›é€‰ï¼š{filtered_before} â†’ {len(filtered)} æ”¯ï¼ˆè¿‡æ»¤{filtered_before - len(filtered)}æ”¯ï¼‰")

                if len(filtered) < 20:
                    logger.warning("âš ï¸ ç­›é€‰åè‚¡ç¥¨è¿‡å°‘ï¼Œä½¿ç”¨å½“æ—¥æ¶¨å¹…ä½œä¸ºç­›é€‰æ¡ä»¶")
                    filtered = df[
                        (df['pe_ttm'].between(cfg['pe'][0], cfg['pe'][1])) &
                        (df['pb'].between(cfg['pb'][0], cfg['pb'][1])) &
                        (df['roe'] >= cfg['roe']) &
                        (df['pct_chg'] <= 6)
                    ]
                    logger.info(f"ğŸ“ˆ æ”¾å®½æ¡ä»¶åï¼š{len(filtered)} æ”¯")

            # 11. æ‰§è¡Œå…¶ä»–ç­›é€‰å’Œæœ€ç»ˆå¤„ç†
            filtered = filtered[~filtered['name'].str.contains('ST|é€€', na=False)]
            logger.info(f"ğŸš« è¿‡æ»¤STåŠé€€å¸‚è‚¡ï¼š {len(filtered)}")

            filtered = filtered[
                ((filtered['market'] == 'ä¸»æ¿') & (filtered['close'] >= 1.5)) |
                ((filtered['market'].isin(['åˆ›ä¸šæ¿', 'ç§‘åˆ›æ¿'])) & (filtered['close'] >= 2))
            ]
            logger.info(f"ğŸ’° æ”¶ç›˜ä»·è¿‡æ»¤ï¼š{len(filtered)}")

            filtered = filtered[filtered['total_mv'].between(*cfg['mv'])]
            logger.info(f"ğŸ¦ å¸‚å€¼åŒºé—´è¿‡æ»¤ï¼š{len(filtered)}")

            filtered = filtered[
                (filtered['turnover_rate'] >= cfg['turnover'][0]) &
                (filtered['turnover_rate'] <= cfg['turnover'][1])
            ]
            logger.info(f"ğŸ”„ æ¢æ‰‹ç‡è¿‡æ»¤ï¼š{len(filtered)}")

            filtered = filtered[filtered['pct_chg'].between(*cfg['pct'])]
            logger.info(f"ğŸ“ˆ æ¶¨è·Œå¹…è¿‡æ»¤ï¼š{len(filtered)}")

            # 12. æœ€ç»ˆå¤„ç†
            filtered = filtered.sort_values('list_date', ascending=False)
            if max_count:
                filtered = filtered.head(max_count)

            logger.info(f"âœ… æœ€ç»ˆç­›é€‰è‚¡ç¥¨æ•°: {len(filtered)}")
            stock_list = list(filtered[['ts_code', 'name']].itertuples(index=False, name=None))
            turnover_map = filtered.set_index('ts_code')['turnover_rate'].to_dict()

            return stock_list, turnover_map

        except Exception as e:
            logger.error(f"get_stock_list error: {e}")
            logger.error(traceback.format_exc())
            return [], {}

    @staticmethod
    def get_k_data(ts_code: str, days: int = 180, end_date: Optional[str] = None) -> Optional[pd.DataFrame]:
        try:
            normal_rate_limiter.wait()
            today = end_date or datetime.today().strftime("%Y%m%d")
            past = (datetime.strptime(today, '%Y%m%d') - timedelta(days=days)).strftime('%Y%m%d')

            df = ts.pro_bar(
                ts_code=ts_code,
                start_date=past,
                end_date=today,
                freq='D',
                asset='E',
                adj='qfq',
                factors=['tor'],
                fields='ts_code,trade_date,open,high,low,close,vol'
            )

            if df is None or df.empty:
                return None

            # âœ… ä¸»åŠ¨æ£€æŸ¥ vol æ˜¯å¦å­˜åœ¨
            if 'vol' not in df.columns:
                logger.warning(f"{ts_code} ç¼ºå¤± vol å­—æ®µï¼Œè·³è¿‡")
                return None

            df = df.sort_values("trade_date")
            df['trade_date'] = pd.to_datetime(df['trade_date'])
            df.set_index("trade_date", inplace=True)

            # âœ… é‡å‘½å vol â†’ volumeï¼Œç»Ÿä¸€ç”¨æ³•
            df.rename(columns={'vol': 'volume'}, inplace=True)

            if len(df) < Config.MIN_DATA_DAYS:
                return None

            return df
        except Exception as e:
            logger.error(f"è·å– {ts_code} æ•°æ®å¤±è´¥: {str(e)}")
            return None

    @staticmethod
    def calculate_technical_indicators(df: pd.DataFrame) -> Tuple[Dict[str, pd.Series], pd.DataFrame]:
        result = {}
        try:
            # === åŸºç¡€æŒ‡æ ‡ ===
            close = df["close"]
            high = df["high"]
            low = df["low"]
            open_price = df["open"]
            vol = df["volume"]

            # åŠ¨æ€å‡çº¿ç³»ç»Ÿ
            windows = [5, 10, 20, 30, 60]
            for w in windows:
                df[f'ma{w}'] = close.rolling(w).mean()

            df['vol_ratio'] = vol / vol.rolling(20).mean()

            # === MACDæŒ‡æ ‡è®¡ç®— ===
            ema12 = close.ewm(span=12, adjust=False).mean()
            ema26 = close.ewm(span=26, adjust=False).mean()
            dif = ema12 - ema26
            dea = dif.ewm(span=9, adjust=False).mean()
            macd = (dif - dea) * 2

            # === å¸ƒæ—å¸¦æŒ‡æ ‡ ===
            df['boll_mid'] = close.rolling(20).mean()
            df['boll_std'] = close.rolling(20).std()
            df['boll_upper'] = df['boll_mid'] + 2 * df['boll_std']
            df['boll_lower'] = df['boll_mid'] - 2 * df['boll_std']

            # === RSIæŒ‡æ ‡ ===
            delta = close.diff()
            gain = delta.where(delta > 0, 0)
            loss = -delta.where(delta < 0, 0)
            avg_gain = gain.rolling(14).mean()
            avg_loss = loss.rolling(14).mean()
            rs = avg_gain / (avg_loss + 1e-6)
            df['rsi'] = 100 - (100 / (1 + rs))

            # === ç›¸å¯¹ä½ç½®è®¡ç®—ï¼ˆçŸ­æœŸä¸ºä¸»ï¼‰===
            high_20d = high.rolling(20).max()
            low_20d = low.rolling(20).min()
            position_20d = (close - low_20d) / (high_20d - low_20d + 0.001)

            # è¶…çŸ­çº¿â€œåº•éƒ¨â€åˆ¤æ–­ï¼ˆæ›´å®½æ¾ï¼‰
            is_low_position = position_20d < 0.65
            oversold = df['rsi'] < 55
            below_boll_mid = close < df['boll_mid']
            at_bottom = is_low_position | oversold | below_boll_mid

            # === ä¸€é˜³ç©¿ä¸‰çº¿ï¼ˆçŸ­çº¿ï¼‰ ===
            is_yang = close > open_price
            cross_today = (close > df['ma5']) & (close > df['ma10']) & (close > df['ma20'])
            prev_below_ma = (
                (close.shift(1) < df['ma5'].shift(1)) |
                (close.shift(1) < df['ma10'].shift(1)) |
                (close.shift(1) < df['ma20'].shift(1))
            )
            volume_increase = vol > vol.rolling(5).mean() * 1.05
            yang_cross_three_line = is_yang & cross_today & prev_below_ma & at_bottom & volume_increase

            # === OBVåŠ¨é‡ ===
            df['obv'] = (np.sign(close.diff()) * vol).fillna(0).cumsum()
            df['obv_ma'] = df['obv'].rolling(20).mean()

            # === ä¿¡å·ç”Ÿæˆ ===
            result = {
                # === è¶‹åŠ¿å‹ ===
                "å‡çº¿çªç ´ï¼ˆ5/20/30æ—¥ï¼‰": (close > df[['ma5', 'ma20', 'ma30']].max(axis=1)),
                "å‡çº¿å¤šå¤´æ’åˆ—": (df['ma5'] > df['ma20']) & (df['ma20'] > df['ma30']),
                "MACDé›¶è½´å…±æŒ¯": (dif > 0) & (dea > 0) & (dif > dea),
                "è¶‹åŠ¿çªç ´ç¡®è®¤": (close > high.rolling(5).max()) & (vol > vol.rolling(5).mean() * 1.5),

                # === åŠ¨é‡å‹ ===
                "é‡ä»·é½å‡": (df['vol_ratio'].between(1.5, 3)) & (close > close.shift(3) * 1.05),
                "ä¸»åŠ›èµ„é‡‘å…±æŒ¯": (macd > 0) & (df['vol_ratio'] > 1.8),
                "OBVåŠ¨é‡å¼•æ“": (df['obv'] > df['obv_ma']) & (close > close.shift(5) * 1.03),

                # === åè½¬å‹ ===
                "è¶…è·Œåå¼¹ï¼ˆRSI+BOLLï¼‰": (close < df['boll_lower']) & (df['rsi'] < 30),
                "åº•éƒ¨åè½¬ç¡®è®¤": (close < df['boll_lower'] * 0.98) & (vol > vol.rolling(5).mean() * 1.2) & (df['rsi'] < 35),

                # === é£é™©å‹ ===
                "è¶‹åŠ¿ç ´ä½ï¼ˆMA60+MACDæ­»å‰ï¼‰": (close < df['ma60']) & (dif < dea),
                "é«˜ä½æ»æ¶¨é£é™©": (close > df['boll_upper']) & (df['rsi'] > 70) & (vol < vol.rolling(5).mean() * 0.8),

                # === ç©¿çº¿å‹ ===
                "ä¸€é˜³ç©¿ä¸‰çº¿": yang_cross_three_line
            }

            return result, df

        except Exception as e:
            import logging
            logger = logging.getLogger(__name__)
            logger.error(f"æŒ‡æ ‡è®¡ç®—é”™è¯¯: {str(e)}")
            return {}, df

    @staticmethod
    def check_risk(df: pd.DataFrame) -> bool:
        return True

class MarketNeutralAnalyzer:
    @staticmethod
    def calculate_relative_strength(ts_code, trade_date, lookback=20):
        """è®¡ç®—ç›¸å¯¹å¼ºå¼±å¾—åˆ†(-50åˆ°50)"""
        try:
            # è·å–è‚¡ç¥¨å’ŒåŸºå‡†æŒ‡æ•°è¿‡å»lookbackå¤©çš„æ”¶ç›Šç‡
            stock_ret = MarketNeutralAnalyzer._get_stock_returns(ts_code, trade_date, lookback)
            bench_ret = MarketNeutralAnalyzer._get_benchmark_returns(trade_date, lookback)
            
            # è®¡ç®—ç›¸å¯¹å¼ºå¼±
            relative_strength = (stock_ret - bench_ret).mean()
            return min(max(relative_strength * 100, -50), 50)  # é™åˆ¶åœ¨-50åˆ°50ä¹‹é—´
        except Exception as e:
            logger.error(f"è®¡ç®—ç›¸å¯¹å¼ºå¼±å¤±è´¥ {ts_code}: {str(e)}")
            return 0

    @staticmethod
    def _get_stock_returns(ts_code, trade_date, lookback):
        """è·å–è‚¡ç¥¨æ”¶ç›Šç‡"""
        end_date = datetime.strptime(trade_date, '%Y%m%d')
        start_date = end_date - timedelta(days=lookback)
        
        df = safe_api_call(
            pro.daily,
            ts_code=ts_code,
            start_date=start_date.strftime('%Y%m%d'),
            end_date=end_date.strftime('%Y%m%d'),
            fields='trade_date,pct_chg'
        )
        return df['pct_chg'] / 100 if not df.empty else pd.Series([0]*lookback)

    @staticmethod
    def _get_benchmark_returns(trade_date, lookback, benchmark='000300.SH'):
        """è·å–åŸºå‡†æŒ‡æ•°æ”¶ç›Šç‡"""
        end_date = datetime.strptime(trade_date, '%Y%m%d')
        start_date = end_date - timedelta(days=lookback)
        
        df = safe_api_call(
            pro.index_daily,
            ts_code=benchmark,
            start_date=start_date.strftime('%Y%m%d'),
            end_date=end_date.strftime('%Y%m%d'),
            fields='trade_date,pct_chg'
        )
        return df['pct_chg'] / 100 if not df.empty else pd.Series([0]*lookback)


class RecommendationTracker:
    def __init__(self):
        self.data_file = "recommendations.pkl"
        try:
            self.recommendations = pd.read_pickle(self.data_file)
            logger.info(f"âœ… æˆåŠŸåŠ è½½æ¨èå†å²ï¼Œå…± {len(self.recommendations)} æ¡è®°å½•")
        except:
            logger.info("ğŸ“‚ æ— å†å²è®°å½•ï¼Œåˆ›å»ºæ–°æ–‡ä»¶")
            self.recommendations = pd.DataFrame()

    def add_recommendation(self, stock_data: dict, recommend_date: Optional[str] = None):
        if self.stock_exists(stock_data['ts_code']):
            logger.info(f"âš ï¸ {stock_data['ts_code']} å·²å­˜åœ¨æ¨èè®°å½•")
            return False

        stock_data['recommend_date'] = recommend_date or datetime.today().strftime('%Y-%m-%d')

        new_record = pd.DataFrame([stock_data])
        self.recommendations = pd.concat([self.recommendations, new_record], ignore_index=True)
        self._save_data()
        logger.info(f"âœ… å·²æ·»åŠ æ¨è: {stock_data['ts_code']} ({stock_data['recommend_date']})")
        return True

    def export_to_watchlist(self):
        # åˆ é™¤å¯¼å‡ºwatchlistæ–‡ä»¶çš„åŠŸèƒ½
        logger.info("ğŸ“ å¯¼å‡ºwatchliståŠŸèƒ½å·²ç¦ç”¨")
        return

    def stock_exists(self, ts_code: str) -> bool:
        if self.recommendations.empty:
            return False
        return ts_code in self.recommendations['ts_code'].values

    def remove_stock(self, ts_code: str):
        if self.recommendations.empty:
            logger.info("âš ï¸ å½“å‰æ— è®°å½•å¯åˆ é™¤")
            return
        before_count = len(self.recommendations)
        self.recommendations = self.recommendations[self.recommendations['ts_code'] != ts_code]
        self._save_data()
        after_count = len(self.recommendations)
        logger.info(f"ğŸ—‘ï¸ åˆ é™¤å®Œæˆï¼Œè®°å½•æ•°: {before_count} -> {after_count}")

    def clear(self):
        self.recommendations = pd.DataFrame()

    def _save_data(self):
        self.recommendations.to_pickle(self.data_file)


# =====================
# å®ä¾‹åŒ– tracker
# =====================

tracker = RecommendationTracker()

def calculate_position(score: float, pct_change: float = 0.0, risk_warnings: List[str] = None, strategy_mode: str = "ç¨³å¥å‹") -> str:
    if risk_warnings is None:
        risk_warnings = []

    # âŒ å¼ºåˆ¶è¿‡æ»¤æç«¯æ¶¨å¹…
    # æ ¹æ®ç­–ç•¥ç±»å‹è°ƒæ•´æ¶¨å¹…é™åˆ¶
    if strategy_mode == "ç¨³å¥å‹":
        max_pct_change = 9.5  # ç¨³å¥å‹é™åˆ¶æ›´ä¸¥æ ¼
    elif strategy_mode == "ç©¿çº¿å‹":
        max_pct_change = 12.0  # ç©¿çº¿å‹å…è®¸è¾ƒé«˜æ¶¨å¹…
    else:
        max_pct_change = 15.0  # æ¿€è¿›å‹å…è®¸æ›´é«˜çš„æ¶¨å¹…
        
    if pct_change >= max_pct_change:
        return "âŒ ä¸å»ºè®®ä¹°å…¥"

    # ğŸš¨ é«˜æ³¢åŠ¨è­¦å‘Šå‡ä»“æˆ–å‰”é™¤
    volatility_penalty = 0.5 if "é«˜æ³¢åŠ¨" in risk_warnings else 1.0
    if volatility_penalty < 1.0:
        return "âŒ ä¸å»ºè®®ä¹°å…¥"  # æ³¢åŠ¨è¿‡å¤§ï¼Œç›´æ¥å‰”é™¤

    # ğŸ“ˆ æ ¹æ®ç­–ç•¥è°ƒæ•´ä»“ä½åˆ†é…
    if strategy_mode == "ç¨³å¥å‹":
        # ç¨³å¥å‹ç­–ç•¥è¾ƒä½çš„ä»“ä½åˆ†é…
        if score >= 150:
            base_position = 0.12
        elif score >= 145:
            base_position = 0.10
        elif score >= 140:
            base_position = 0.09
        elif score >= 135:
            base_position = 0.08
        elif score >= 130:
            base_position = 0.07
        elif score >= 125:
            base_position = 0.06
        elif score >= 120:
            base_position = 0.05
        elif score >= 115:
            base_position = 0.04
        elif score >= 110:
            base_position = 0.03
        else:
            return "âŒ ä¸å»ºè®®ä¹°å…¥"
    
    elif strategy_mode == "ç©¿çº¿å‹":
        # ç©¿çº¿å‹ç­–ç•¥çš„ä»“ä½åˆ†é…
        if score >= 180:
            base_position = 0.15
        elif score >= 160:
            base_position = 0.13
        elif score >= 150:
            base_position = 0.12
        elif score >= 140:
            base_position = 0.10
        elif score >= 130:
            base_position = 0.08
        elif score >= 120:
            base_position = 0.06
        elif score >= 110:
            base_position = 0.04
        elif score >= 100:
            base_position = 0.03
        else:
            return "âŒ ä¸å»ºè®®ä¹°å…¥"

    elif strategy_mode == "æ¿€è¿›å‹":
        # æ¿€è¿›å‹ç­–ç•¥å…è®¸è¾ƒé«˜çš„ä»“ä½åˆ†é…
        if score >= 200:
            base_position = 0.15
        elif score >= 180:
            base_position = 0.14
        elif score >= 170:
            base_position = 0.13
        elif score >= 160:
            base_position = 0.12
        elif score >= 150:
            base_position = 0.11
        elif score >= 140:
            base_position = 0.10
        elif score >= 130:
            base_position = 0.09
        elif score >= 125:
            base_position = 0.08
        elif score >= 120:
            base_position = 0.07
        elif score >= 115:
            base_position = 0.06
        elif score >= 110:
            base_position = 0.05
        elif score >= 105:
            base_position = 0.04
        elif score >= 100:
            base_position = 0.03
        else:
            return "âŒ ä¸å»ºè®®ä¹°å…¥"

    final_position = base_position * volatility_penalty

    # ğŸ§¾ è½¬æ¢ä¸ºæ–‡æœ¬æ ‡ç­¾ï¼Œé€‚åº”ä¸åŒç­–ç•¥
    if final_position >= 0.15:
        if strategy_mode == "æ¿€è¿›å‹":
            return "15%-20%"
        elif strategy_mode == "ç©¿çº¿å‹":
            return "15%-18%"
        else:
            return "12%-15%"
    elif final_position >= 0.10:
        if strategy_mode == "æ¿€è¿›å‹":
            return "10%-15%" 
        elif strategy_mode == "ç©¿çº¿å‹":
            return "10%-13%"
        else:
            return "8%-12%"
    elif final_position >= 0.05:
        return "5%-8%"
    elif final_position >= 0.01:
        return "â‰¤5%"
    else:
        return "âš ï¸ ä»“ä½è¿‡å°"







# ===== ç•Œé¢ç›¸å…³å‡½æ•° =====
def get_tracking_html():
    html = "<h3>ğŸ“Š æ¨èå†å²è®°å½•</h3><ul>"
    if tracker.recommendations.empty:
        return "<h3>ğŸ“­ æš‚æ— æ¨èè®°å½•</h3>"

    for _, row in tracker.recommendations.iterrows():
        html += f"<li>{row['recommend_date']} - {row['name']} ({row['ts_code']})</li>"
    html += "</ul>"
    return html



HIGH_VOLATILITY_INDUSTRIES = [
    "é€šä¿¡è®¾å¤‡", "åŠå¯¼ä½“", "æ–°èƒ½æº", "ç”Ÿç‰©åŒ»è¯", "å†›å·¥", "æ¸¸æˆ", "åˆ›ä¸šæ¿"
]

@lru_cache(maxsize=5000)
def is_high_volatility_industry(ts_code: str) -> bool:
    try:
        basic_info = safe_api_call(pro.stock_basic, ts_code=ts_code, fields='ts_code,industry')
        if basic_info.empty:
            return False
        industry = basic_info.iloc[0]['industry']
        return industry in HIGH_VOLATILITY_INDUSTRIES
    except Exception as e:
        logger.warning(f"è¡Œä¸šæŸ¥è¯¢å¤±è´¥ {ts_code}: {str(e)}")
        return False

@lru_cache(maxsize=1000)
def check_negative_announcements(ts_code: str) -> bool:
    # æš‚æ— æƒé™ï¼Œç›´æ¥è·³è¿‡
    return False
    try:
        start_date = (datetime.today() - timedelta(days=7)).strftime('%Y%m%d')
        df = safe_api_call(pro.anns_d, ts_code=ts_code, start_date=start_date)
        if df.empty:
            return False
        negative_keywords = ['è¯‰è®¼', 'ä»²è£', 'ç«‹æ¡ˆè°ƒæŸ¥', 'é¡¹ç›®ç»ˆæ­¢', 'æ§åˆ¶æƒå˜æ›´', 'é‡å¤§é£é™©']
        return df['title'].str.contains('|'.join(negative_keywords)).any()
    except Exception as e:
        logger.warning(f"{ts_code} å…¬å‘Šæ£€æŸ¥å¤±è´¥: {str(e)}")
        return False



@lru_cache(maxsize=1000)
def check_earnings_warning(ts_code: str) -> bool:
    try:
        current_year = datetime.today().year
        df = safe_api_call(pro.forecast_vip, ts_code=ts_code, start_date=f"{current_year}0101")
        if df.empty:
            return False
        return df['type'].str.contains("é¢„å‡|é¢„äº|ç»­äº").any()
    except Exception as e:
        logger.warning(f"{ts_code} é¢„è­¦æ£€æŸ¥å¤±è´¥: {str(e)}")
        return False








import warnings
warnings.simplefilter(action='ignore', category=FutureWarning)

def get_type_weight_safe(strategy_type: str, type_weights: Dict[str, float]) -> float:
    """å®‰å…¨è·å–ç­–ç•¥ç±»å‹æƒé‡ï¼Œä¸‰é‡å…œåº•"""
    return type_weights.get(strategy_type) if type_weights.get(strategy_type) is not None else STRATEGY_TYPE_WEIGHTS.get(strategy_type, 1.0)


def evaluate_yang_cross_strength(df: pd.DataFrame) -> str:
    """
    è¯„ä¼°ä¸€é˜³ç©¿ä¸‰çº¿ä¿¡å·çš„è´¨é‡ï¼Œè¿”å›è¯„çº§æ–‡æœ¬ã€‚
    """
    close = df['close'].iloc[-1]
    open_price = df['open'].iloc[-1]
    high = df['high'].iloc[-1]
    low = df['low'].iloc[-1]
    vol = df['volume'].iloc[-1]
    ma5 = df['ma5'].iloc[-1]
    ma10 = df['ma10'].iloc[-1]
    ma20 = df['ma20'].iloc[-1]

    # å®ä½“å¼ºåº¦ï¼šé˜³çº¿é•¿åº¦
    body_pct = (close - open_price) / (open_price + 1e-6)

    # å‡çº¿å‘æ•£ç»“æ„
    trend_order = (ma5 > ma10) and (ma10 > ma20)

   
    recent_high = df['high'].rolling(10).max().iloc[-1]
    space_pct = (recent_high - close) / (close + 1e-6)

    # æˆäº¤é‡æ”¾å¤§ç¨‹åº¦
    vol_mean = df['volume'].rolling(5).mean().iloc[-1]
    vol_ratio = vol / (vol_mean + 1e-6)

    score = 0
    if body_pct > 0.02: score += 1
    if trend_order: score += 1
    if space_pct > 0.05: score += 1
    if vol_ratio > 1.5: score += 1

    if score >= 3:
        return "ğŸ”¥é«˜è´¨é‡ç©¿çº¿"
    elif score == 2:
        return "âš ï¸ä¸­ç­‰ç©¿çº¿"
    else:
        return "âŒå¼±ç©¿çº¿"

def analyze_stocks(stock_list_with_turnover: Tuple[List[Tuple[str, str]], Dict[str, float]],
                   strategies: List[str],
                   custom_weights: Dict[str, int],
                   max_stocks: int,
                   strategy_mode: str,
                   trade_date: Optional[str] = None,
                   export_watchlist: bool = True,
                   top_n: int = 10) -> List[Tuple]:
    stock_list, turnover_map = stock_list_with_turnover

    if trade_date:
        trade_date = datetime.strptime(trade_date, '%Y%m%d')
    else:
        trade_date = datetime.today()

    actual_trade_date = get_valid_trade_date(api_func=pro.daily, date_field='trade_date', base_date=trade_date)
    if not actual_trade_date:
        logger.error("âŒ æ— æ³•è·å–æœ‰æ•ˆäº¤æ˜“æ—¥ï¼Œç»“æŸåˆ†æï¼")
        return []

    logger.info(f"ğŸš€ åˆ†æå¯åŠ¨ï¼š{len(stock_list)} æ”¯è‚¡ç¥¨ ï½œ æ¨¡å¼ï¼š{strategy_mode} ï½œ æ•°æ®æ—¥æœŸï¼š{actual_trade_date}")
    
    # âœ… æ¿å—çƒ­åº¦è¯„åˆ†ç³»ç»Ÿé›†æˆéƒ¨åˆ†
    if isinstance(actual_trade_date, datetime):
        trade_date_str = actual_trade_date.strftime('%Y%m%d')
    else:
        trade_date_str = actual_trade_date  # è‹¥å·²ä¸ºå­—ç¬¦ä¸²ï¼Œç›´æ¥ä½¿ç”¨
    
    # ä¿®æ”¹ï¼šå›æµ‹æ¨¡å¼ä¸‹è·³è¿‡æ¿å—è¯„åˆ†
    if IS_BACKTEST:
        logger.info(f"ğŸ”™ å›æµ‹æ¨¡å¼ï¼šè·³è¿‡æ¿å—çƒ­åº¦è¯„åˆ†")
        sector_scores = {}
        concept_map = {}
        concept_name_to_ts_code = {}
    else:
        logger.info(f"ğŸ“… å¼€å§‹è·å–æ¿å—æ•°æ®ï¼Œæ—¥æœŸ: {trade_date_str}")
        
        sector_scores = get_sector_strength_scores(trade_date_str)
        logger.info(f"ğŸ“Š æ¿å—è¯„åˆ†ç»“æœæ•°é‡: {len(sector_scores)}")
        if sector_scores:
            # æ‰“å°å‰5ä¸ªæ¿å—çš„è¯„åˆ†
            sample_scores = dict(list(sector_scores.items())[:5])
            logger.info(f"ğŸ“Š æ¿å—è¯„åˆ†æ ·ä¾‹: {sample_scores}")
        else:
            logger.warning("âš ï¸ æ¿å—è¯„åˆ†ä¸ºç©ºï¼å¯èƒ½æ¥å£è°ƒç”¨å¤±è´¥")
        
        concept_map = load_concept_to_stock_map()
        logger.info(f"ğŸ“š åŠ è½½æ¦‚å¿µè‚¡ç¥¨æ˜ å°„: {len(concept_map)} ä¸ªæ¦‚å¿µ")
        if concept_map:
            # æ‰“å°ä¸€ä¸ªæ ·ä¾‹
            sample_concept = list(concept_map.keys())[0]
            sample_stocks = concept_map[sample_concept][:3]
            logger.info(f"ğŸ“š æ¦‚å¿µæ˜ å°„æ ·ä¾‹: {sample_concept} -> {sample_stocks}")
        
        concept_name_to_ts_code = load_concept_name_to_code()  # ä»æ–‡ä»¶åŠ è½½æ˜ å°„
        logger.info(f"ğŸ”— åŠ è½½æ¦‚å¿µä»£ç æ˜ å°„: {len(concept_name_to_ts_code)} ä¸ªæ˜ å°„")
        if concept_name_to_ts_code:
            # æ‰“å°å‰3ä¸ªæ˜ å°„
            sample_mappings = dict(list(concept_name_to_ts_code.items())[:3])
            logger.info(f"ğŸ”— æ¦‚å¿µä»£ç æ˜ å°„æ ·ä¾‹: {sample_mappings}")
        
        # å¦‚æœæ–‡ä»¶ä¸å­˜åœ¨ï¼Œæç¤ºç”¨æˆ·è¿è¡Œæ„å»ºè„šæœ¬
        if not concept_map or not concept_name_to_ts_code:
            logger.warning("âš ï¸ æ¦‚å¿µæ˜ å°„æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè¯·å…ˆè¿è¡Œ build_concept_mapping.py æ„å»ºæ˜ å°„")
            concept_name_to_ts_code = {}

    if not IS_BACKTEST:
        concept_data = get_concept_trends(trade_date)
        concept_trend_score = calculate_concept_trend_score(concept_data, trade_date)
    else:
        concept_trend_score = 0  # å›æµ‹æ¨¡å¼ä¸‹ï¼Œè·³è¿‡æ¦‚å¿µè¶‹åŠ¿å¾—åˆ†çš„è®¡ç®—ï¼Œè®¾ç½®ä¸º0
        
    scored_stocks = []
    ts_codes = [ts_code for ts_code, _ in stock_list]

    # Initialize various data
    initialize_industry_and_concept(ts_codes)
    initialize_moneyflow_scores(ts_codes)
    initialize_risk_data(ts_codes)
    initialize_stk_limit(ts_codes)
    initialize_block_trade(ts_codes)
    initialize_top_inst()
    initialize_share_float_data(ts_codes)
    initialize_holdernumber_data(ts_codes)
   
    current_suspend = set(pro.suspend_d(trade_date=actual_trade_date)['ts_code'].str.upper())
    ts_codes = [code for code in ts_codes if code not in current_suspend]

    # è·å–å¸‚åœºçŠ¶æ€è°ƒæ•´åçš„æƒé‡
    type_weights = STRATEGY_TYPE_WEIGHTS.copy()

    if strategy_mode == "ç¨³å¥å‹":
        type_weights["è¶‹åŠ¿å‹"] *= 1.2
        type_weights["åŠ¨é‡å‹"] *= 0.8
        type_weights["åè½¬å‹"] *= 0.9
        type_weights["å¸‚åœºä¸­æ€§å‹"] *= 1.2
        type_weights["é£é™©å‹"] = max(-2.0, type_weights.get("é£é™©å‹", -1.2) * 1.4)
    elif strategy_mode == "æ¿€è¿›å‹":
        type_weights["è¶‹åŠ¿å‹"] *= 0.9
        type_weights["åŠ¨é‡å‹"] *= 1.15
        type_weights["åè½¬å‹"] *= 1.1
        type_weights["å¸‚åœºä¸­æ€§å‹"] *= 0.75
        type_weights["é£é™©å‹"] = max(-3.0, type_weights.get("é£é™©å‹", -1.0) * 0.8)
    elif strategy_mode == "ç©¿çº¿å‹":
        # ç©¿çº¿å‹ç­–ç•¥ç‰¹æ®Šè°ƒæ•´
        type_weights["ç©¿çº¿å‹"] *= 1.5  
        type_weights["è¶‹åŠ¿å‹"] *= 0.8
        type_weights["åŠ¨é‡å‹"] *= 0.8
        type_weights["é£é™©å‹"] = max(-2.0, type_weights.get("é£é™©å‹", -1.0) * 0.6)

    # Step 2: å†æ ¹æ®å¸‚åœºçŠ¶æ€åŠ¨æ€è°ƒæ•´æƒé‡
    if IS_BACKTEST and custom_weights:
        logger.info("ğŸ¯ ä½¿ç”¨å›æµ‹ä¼ å…¥çš„å¸‚åœºæ‰°åŠ¨æƒé‡")
        for key, market_weight in custom_weights.items():
            type_weights[key] = type_weights.get(key, 1.0) * float(market_weight)
    else:
        market_weights = adjust_strategy_weights_by_market(trade_date=actual_trade_date)
        logger.info("ğŸ“¡ ä½¿ç”¨å®æ—¶å¸‚åœºæ‰°åŠ¨æƒé‡")
        for key, market_weight in market_weights.items():
            type_weights[key] = type_weights.get(key, 1.0) * float(market_weight)

    # Step 3: æœ€ç»ˆåˆå¹¶æƒé‡
    merged_weights = type_weights
    logger.info(f"âš–ï¸ æœ€ç»ˆç­–ç•¥æƒé‡ (ç­–ç•¥æ¨¡å¼: {strategy_mode}):\n" +
                "\n".join([f"- {k}: {v:.2f}" for k, v in merged_weights.items()]))

    daily_info_df = safe_api_call(pro.daily, trade_date=actual_trade_date, fields='ts_code,pct_chg')
    
    # æ£€æŸ¥ 'pct_chg' åˆ—æ˜¯å¦å­˜åœ¨
    if 'pct_chg' in daily_info_df.columns:
        pct_chg_map = daily_info_df.set_index('ts_code')['pct_chg'].to_dict()
    else:
        logger.warning("âŒ 'pct_chg' åˆ—åœ¨æ•°æ®æ¡†ä¸­æœªæ‰¾åˆ°")
        pct_chg_map = {}  # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ï¼Œè¿”å›ç©ºå­—å…¸

    removal_stats = defaultdict(int)

    # è®°å½•å·²å¤„ç†çš„è‚¡ç¥¨ï¼Œé¿å…é‡å¤
    seen_stocks = set()

    def process_stock(ts_code_name):
        ts_code, name = ts_code_name
        try:
            # æ£€æŸ¥æ˜¯å¦å·²ç»å¤„ç†è¿‡è¯¥è‚¡ç¥¨ï¼Œé¿å…é‡å¤
            if ts_code in seen_stocks:
                return None
            seen_stocks.add(ts_code)  # æ ‡è®°è¯¥è‚¡ç¥¨å·²å¤„ç†

            if check_earnings_warning(ts_code):
                removal_stats["é£é™©é¢„è­¦"] += 1
                logger.info(f"ğŸ›‘ {ts_code} è¢«ç­›é™¤ï¼ŒåŸå› ï¼šé£é™©é¢„è­¦")
                return None

            df = StockAnalyzer.get_k_data(ts_code, days=60, end_date=actual_trade_date)
            if df is None or len(df) < Config.MIN_DATA_DAYS:
                removal_stats["æ•°æ®ä¸è¶³"] += 1
                logger.info(f"ğŸ›‘ {ts_code} è¢«ç­›é™¤ï¼ŒåŸå› ï¼šæ•°æ®ä¸è¶³")
                return None

            indicators, df = StockAnalyzer.calculate_technical_indicators(df)

            financial_score = evaluate_financials(ts_code)
            score = 0
            score_details = {}
            score += financial_score * 1
            score_details['åŸºæœ¬é¢å¾—åˆ†'] = financial_score * 1

            # ç­–ç•¥åŒ¹é…è¿‡æ»¤
            raw_matched = list(set(
                [s for s in strategies if s in indicators and indicators[s].iloc[-1]] +
                [s for s in POTENTIAL_SIGNALS if s in indicators and indicators[s].iloc[-1]]
            ))

            # å®šä¹‰æœ‰æ•ˆç­–ç•¥ç™½åå•
            valid_strategies = list(STRATEGY_WEIGHTS.keys()) + POTENTIAL_SIGNALS
            matched = [s for s in raw_matched if s in valid_strategies]

            if strategy_mode == "ç©¿çº¿å‹":
                # æ£€æŸ¥æ˜¯å¦æ»¡è¶³"ä¸€é˜³ç©¿ä¸‰çº¿"æŒ‡æ ‡
                if "ä¸€é˜³ç©¿ä¸‰çº¿" in indicators and indicators["ä¸€é˜³ç©¿ä¸‰çº¿"].iloc[-1]:
                    # âœ… æ–°å¢ï¼šç©¿çº¿è´¨é‡åˆ¤æ–­ï¼Œæå‰ç­›é™¤å¼±ä¿¡å·
                    quality = evaluate_yang_cross_strength(df)
                    if quality == "âŒå¼±ç©¿çº¿":
                        removal_stats["ç©¿çº¿ä¿¡å·å¼±"] += 1
                        logger.info(f"ğŸ›‘ {ts_code} è¢«ç­›é™¤ï¼ŒåŸå› ï¼šç©¿çº¿ä¿¡å·å¼ºåº¦ä¸è¶³ï¼ˆå¼±ç©¿çº¿ï¼‰")
                        return None

                    # å¦‚æœæ»¡è¶³ï¼Œå¼ºåˆ¶æ·»åŠ åˆ°åŒ¹é…ç­–ç•¥ä¸­
                    if "ä¸€é˜³ç©¿ä¸‰çº¿" not in matched:
                        matched.append("ä¸€é˜³ç©¿ä¸‰çº¿")
                    # ç»™äºˆé¢å¤–åŠ åˆ†
                    score += 30
                    score_details['ç©¿çº¿å‹åŠ åˆ†'] = 30

                    # å†™å…¥è¯„åˆ†ç­‰çº§
                    score_details['ç©¿çº¿è¯„åˆ†'] = quality
                    if quality == "ğŸ”¥é«˜è´¨é‡ç©¿çº¿":
                        score += 15
                    elif quality == "âš ï¸ä¸­ç­‰ç©¿çº¿":
                        score += 0  # ä¿ç•™ä¸­ç­‰ï¼Œä¸é¢å¤–åŠ åˆ†

                elif not matched:
                    # å¦‚æœæ˜¯ç©¿çº¿å‹ç­–ç•¥æ¨¡å¼ä½†ä¸æ»¡è¶³ä¸€é˜³ç©¿ä¸‰çº¿ä¸”æ²¡æœ‰å…¶ä»–åŒ¹é…ç­–ç•¥ï¼Œåˆ™è·³è¿‡
                    removal_stats["ä¸æ»¡è¶³ç©¿çº¿æ¡ä»¶"] += 1
                    logger.info(f"ğŸ›‘ {ts_code} è¢«ç­›é™¤ï¼ŒåŸå› ï¼šç©¿çº¿å‹ç­–ç•¥æ¨¡å¼ä¸‹ä¸æ»¡è¶³ç©¿çº¿æ¡ä»¶")
                    return None

            if not matched:
                removal_stats["æ— åŒ¹é…ç­–ç•¥"] += 1
                logger.info(f"ğŸ›‘ {ts_code} è¢«ç­›é™¤ï¼ŒåŸå› ï¼šæ— åŒ¹é…ç­–ç•¥")
                return None

            # æƒé‡è®¡ç®—åŠç­–ç•¥å¾—åˆ†
            weights = {s: STRATEGY_WEIGHTS[s] * merged_weights.get(get_strategy_type(s), 1.0) for s in STRATEGY_WEIGHTS}
            weights.update(custom_weights)

            matched = sorted(matched, key=lambda s: weights.get(s, 0), reverse=True)[:3]
            matched = [s for s in matched if get_strategy_type(s) != "é£é™©å‹"]

            tech_score = 0
            score_details['æŠ€æœ¯é¢å¾—åˆ†ç»†èŠ‚'] = []

            for s in matched:
                # è·å–ç­–ç•¥çš„ç±»å‹å¹¶æ ¹æ®åŠ¨æ€æƒé‡è¿›è¡ŒåŠ æƒ
                strategy_type = get_strategy_type(s)
                strategy_score = weights.get(s, 10) * merged_weights.get(strategy_type, 1.0)

                tech_score += strategy_score
                score_details['æŠ€æœ¯é¢å¾—åˆ†ç»†èŠ‚'].append(f"{s}: {strategy_score:.1f}")

            score += tech_score
            score_details['æŠ€æœ¯é¢å¾—åˆ†'] = tech_score

            market_neutral_weight = merged_weights.get("å¸‚åœºä¸­æ€§å‹", 1.0)
            rs_score = MarketNeutralAnalyzer.calculate_relative_strength(ts_code, actual_trade_date)
            neutral_bonus = rs_score * 10
            neutral_bonus_weighted = neutral_bonus * market_neutral_weight * 0.5
            score += neutral_bonus_weighted
            score_details['å¸‚åœºä¸­æ€§å¾—åˆ†'] = neutral_bonus_weighted
            
            # å…¶ä»–è¯„åˆ†é€»è¾‘
            
            old_score = score
            # ä¿®æ”¹ï¼šå›æµ‹æ¨¡å¼ä¸‹è·³è¿‡æ¿å—çƒ­åº¦åŠ åˆ†
            if IS_BACKTEST:
                logger.debug(f"ğŸ”™ å›æµ‹æ¨¡å¼ï¼š{ts_code} è·³è¿‡æ¿å—çƒ­åº¦åŠ åˆ†")
                score_details['æ¿å—çƒ­åº¦åŠ åˆ†'] = 0
            else:
                score = inject_sector_score(score, ts_code, concept_name_to_ts_code, sector_scores, concept_map, weight=0.3)
                score_details['æ¿å—çƒ­åº¦åŠ åˆ†'] = round(score - old_score, 2)

            risk_penalty = evaluate_risk_factors(ts_code)  # é£é™©å› å­
            score -= risk_penalty
            logger.debug(f"ğŸ”´ {ts_code} é£é™©æ‰£åˆ†: {risk_penalty}")

            share_float_penalty = evaluate_share_float(ts_code)  # é™å”®
            score += share_float_penalty
            logger.debug(f"ğŸ’¸ {ts_code} æœªæ¥é™å”®è§£ç¦å¾—åˆ†: {share_float_penalty}")

            holdernumber_score = evaluate_holdernumber(ts_code)  # è‚¡ä¸œäººæ•°å˜åŒ–
            score += holdernumber_score
            logger.debug(f"ğŸ‘¥ {ts_code} è‚¡ä¸œäººæ•°å˜åŒ–å¾—åˆ†: {holdernumber_score}")

            express_score = evaluate_express(ts_code)  # å¿«é€Ÿè´¢æŠ¥è¯„åˆ†
            score += express_score
            logger.debug(f"ğŸ“ˆ {ts_code} å¿«é€Ÿè´¢æŠ¥å¾—åˆ†: {express_score}")

            top_inst_score = check_top_inst(ts_code)  # ä¸»åŠ›èµ„é‡‘è¯„åˆ†
            score += top_inst_score
            logger.debug(f"ğŸ¦ {ts_code} ä¸»åŠ›èµ„é‡‘å¾—åˆ†: {top_inst_score}")
            
            score += concept_trend_score
            score_details['æ¦‚å¿µè¶‹åŠ¿å¾—åˆ†'] = concept_trend_score

            # å…¶ä»–æ‰£åˆ†é€»è¾‘
            current_price = df['close'].iloc[-1]
            limit_info = stk_limit_cache.get(ts_code)
            if limit_info and current_price <= limit_info['down_limit'] * 1.005:
                removal_stats["æ¥è¿‘è·Œåœ"] += 1
                logger.info(f"ğŸ›‘ {ts_code} è¢«ç­›é™¤ï¼ŒåŸå› ï¼šæ¥è¿‘è·Œåœ")
                return None

            turnover = turnover_map.get(ts_code)
            if turnover is None:
                removal_stats["æ¢æ‰‹ç‡è¿‡ä½"] += 1
                logger.info(f"ğŸ›‘ {ts_code} è¢«ç­›é™¤ï¼ŒåŸå› ï¼šæ¢æ‰‹ç‡è¿‡ä½")
                return None
            
            day_volatility = (df['high'].iloc[-1] - df['low'].iloc[-1]) / df['close'].iloc[-2]
            if day_volatility > 0.15:  # å•æ—¥æ³¢åŠ¨è¶…15%
                removal_stats["å¼‚å¸¸æ³¢åŠ¨"] += 1
                return None
            
            # æ ¹æ®ç­–ç•¥æ¨¡å¼è®¾ç½®ä¸åŒçš„æ¢æ‰‹ç‡è¯„åˆ†
            if strategy_mode == "ç¨³å¥å‹":
                turnover_score = 6 if 1.5 <= turnover <= 12 else 4
            elif strategy_mode == "ç©¿çº¿å‹":
                # ç©¿çº¿å‹ç­–ç•¥æ›´çœ‹é‡æ¢æ‰‹ç‡
                turnover_score = 10 if 4 <= turnover <= 20 else 6
            else:
                turnover_score = 8 if 4 <= turnover <= 15 else 4
                
            score += turnover_score
            score_details['æ¢æ‰‹ç‡åŠ åˆ†'] = turnover_score

            # æœ€åè¿”å›å¾—åˆ†
            return (score, ts_code, name, matched, df['close'].pct_change(5).iloc[-1] * 100, df, score_details)

        except Exception as e:
            logger.error(f"{ts_code} åˆ†æå¼‚å¸¸: {e}")
            return None

    # å¤„ç†è‚¡ç¥¨æ•°æ®
    with concurrent.futures.ThreadPoolExecutor(max_workers=Config.MAX_WORKERS) as executor:
        futures = [executor.submit(process_stock, stock) for stock in stock_list[:max_stocks]]
        for future in concurrent.futures.as_completed(futures):
            result = future.result()
            if result:
                scored_stocks.append(result)

    # ç»“æœç»Ÿè®¡å’Œå»é‡
    logger.info(f"âœ… åˆ†æå®Œæˆï¼šæ€»{len(stock_list)}æ”¯ï¼Œå€™é€‰{len(scored_stocks)}æ”¯")
    for reason, count in removal_stats.items():
        logger.info(f"âš ï¸ è¢«ç­›é™¤çš„åŸå› ç»Ÿè®¡ï¼š{reason}ï¼š{count}æ”¯")

    # æ ¹æ®ç­–ç•¥æ¨¡å¼è°ƒæ•´æ¨èæœºåˆ¶
    if strategy_mode == "ç©¿çº¿å‹":
        # æå–æ»¡è¶³"ä¸€é˜³ç©¿ä¸‰çº¿"æ¡ä»¶çš„è‚¡ç¥¨
        cross_line_stocks = [stock for stock in scored_stocks if "ä¸€é˜³ç©¿ä¸‰çº¿" in stock[3]]
        
        if cross_line_stocks:
            logger.info(f"ğŸ¯ æ‰¾åˆ°æ»¡è¶³ä¸€é˜³ç©¿ä¸‰çº¿çš„è‚¡ç¥¨ï¼š{len(cross_line_stocks)}æ”¯")
            # ä¿®æ”¹ï¼šæ˜¾ç¤ºæ‰€æœ‰å‘½ä¸­çš„è‚¡ç¥¨ï¼Œä¸é™åˆ¶æ•°é‡ï¼Œä½†ä»æŒ‰ç…§å¾—åˆ†æ’åº
            final_stocks = sorted(cross_line_stocks, key=lambda x: x[0], reverse=True)
        else:
            logger.warning("âš ï¸ æœªæ‰¾åˆ°æ»¡è¶³ä¸€é˜³ç©¿ä¸‰çº¿æ¡ä»¶çš„è‚¡ç¥¨ï¼Œå°†ä½¿ç”¨æ™®é€šæ’åº")
            final_stocks = sorted(scored_stocks, key=lambda x: x[0], reverse=True)[:top_n]
    elif strategy_mode == "ç¨³å¥å‹":
        final_stocks = diversify_recommendations(scored_stocks, max_recommend=top_n)
    else:
        final_stocks = sorted(scored_stocks, key=lambda x: x[0], reverse=True)[:top_n]

    # å»é‡æ“ä½œï¼šç¡®ä¿æ¯åªè‚¡ç¥¨åªå‡ºç°ä¸€æ¬¡
    final_stocks = list({stock[1]: stock for stock in final_stocks}.values())

    for score, ts_code, name, matched, pct_change, df, score_details in final_stocks:
        logger.info(f"ğŸ‡ {ts_code} {name} æ€»åˆ†: {score:.1f} | ç»„æˆ: {score_details}")

    if export_watchlist:
        tracker.clear()
        # å¯¼å‡ºè‚¡ç¥¨åˆ—è¡¨æ—¶ï¼Œå¦‚æœæ˜¯ç©¿çº¿å‹ç­–ç•¥ä¸”æ•°é‡è¿‡å¤šï¼Œåªå¯¼å‡ºå‰top_nåª
        export_stocks = final_stocks
        if strategy_mode == "ç©¿çº¿å‹" and len(final_stocks) > top_n:
            logger.info(f"ğŸ”„ ç©¿çº¿å‹ç­–ç•¥æ£€æµ‹åˆ°{len(final_stocks)}æ”¯è‚¡ç¥¨ï¼Œä½†åªå¯¼å‡ºå¾—åˆ†æœ€é«˜çš„{top_n}æ”¯")
            export_stocks = final_stocks[:top_n]
            
        for score, ts_code, name, matched, pct_change, df, score_details in export_stocks:
            tracker.add_recommendation({
                'ts_code': ts_code,
                'name': name,
                'strategies': matched,
                'score': score,
                'price': df['close'].iloc[-1],
                'position': calculate_position(score, pct_change, [], strategy_mode),  # æ·»åŠ strategy_modeå‚æ•°
                'is_top': True
            })
        #tracker.export_to_watchlist()

    logger.info(f"ğŸ“¢ æœ€ç»ˆæ¨èï¼š{[ts_code for _, ts_code, *_ in final_stocks]}")
    
    return final_stocks




def chat_interface(user_input: str, market_type: List[str], max_stocks: int, strategy_mode: str, history: List) -> Tuple[List, List]:
    default_trigger_phrases = ["æ¨èè‚¡ç¥¨", "å¸®æˆ‘æ¨è", "é€‰è‚¡", "ç»™æˆ‘æ¨è", "æ‰¾è‚¡ç¥¨"]
    markets_str = ", ".join(market_type)

    # å¯ç”¨å…¨ç­–ç•¥ï¼ˆä¸»åŠ¨å‰”é™¤é£é™©å‹ç­–ç•¥ï¼‰
    strategy_items = [s for s in STRATEGY_WEIGHTS.keys() if get_strategy_type(s) != "é£é™©å‹"]
    custom_weights = {s: w for s, w in STRATEGY_WEIGHTS.items() if get_strategy_type(s) != "é£é™©å‹"}

    if strategy_mode == "ç¨³å¥å‹":
        explanation = "ğŸ“˜ ã€ç¨³å¥å‹ã€‘ï¼šè¶‹åŠ¿å‹ä¸ºä¸»ï¼Œé€‚åº¦ä¿ç•™åŠ¨é‡ä¸åå¼¹ï¼Œä¸¥æ ¼è§„é¿é£é™©ã€‚"
       
    elif strategy_mode == "æ¿€è¿›å‹":
        explanation = "ğŸš€ ã€æ¿€è¿›å‹ã€‘ï¼šçªå‡ºçŸ­çº¿åŠ¨é‡ä¸é‡èƒ½æœºä¼šï¼Œè¶‹åŠ¿é€‚å½“é™ä½ï¼Œé£é™©ç­–ç•¥å·²éš”ç¦»ã€‚"
        
    elif strategy_mode == "ç©¿çº¿å‹":
        explanation = "ğŸŒŸ ã€ç©¿çº¿å‹ã€‘ï¼šä¸“æ³¨äºæ•æ‰å‡çº¿ç©¿è¶Šä¿¡å·ï¼Œä¸»è¦å¯»æ‰¾ä¸€é˜³ç©¿ä¸‰çº¿å½¢æ€ï¼Œæ³¨é‡çªç ´ç¡®è®¤ã€‚"
        # ç©¿çº¿å‹æ¨¡å¼ä¸‹ï¼Œå¼ºåˆ¶æ·»åŠ ä¸€é˜³ç©¿ä¸‰çº¿ç­–ç•¥
        if "ä¸€é˜³ç©¿ä¸‰çº¿" not in strategy_items:
            strategy_items.append("ä¸€é˜³ç©¿ä¸‰çº¿")
            custom_weights["ä¸€é˜³ç©¿ä¸‰çº¿"] = STRATEGY_WEIGHTS.get("ä¸€é˜³ç©¿ä¸‰çº¿", 45)
        
    elif any(phrase in user_input for phrase in default_trigger_phrases):
        explanation = "ğŸ¤– æ³›åŒ–è¯·æ±‚ï¼šå‡è¡¡å¯ç”¨ç­–ç•¥ï¼Œå·²è‡ªåŠ¨å‰”é™¤é£é™©ç­–ç•¥ï¼Œç»¼åˆè¯„ä¼°æœºä¼šã€‚"
    else:
        response = DeepSeekAPI.call_deepseek(user_input)
        strategy_items, explanation, custom_weights = DeepSeekAPI.parse_strategies(response)
        # å†æ¬¡è¿‡æ»¤é£é™©å‹ç­–ç•¥
        strategy_items = [s for s in strategy_items if get_strategy_type(s) != "é£é™©å‹"]
        custom_weights = {s: w for s, w in custom_weights.items() if get_strategy_type(s) != "é£é™©å‹"}

        if not strategy_items:
            error_msg = f"âš ï¸ æœªè¯†åˆ«åˆ°æœ‰æ•ˆç­–ç•¥\n{explanation}"
            history.append((user_input, error_msg))
            return history, history

    try:
        logger.info(f"ğŸ“‹ å¯ç”¨ç­–ç•¥æ•°ï¼š{len(strategy_items)}ï¼Œå¸‚åœºï¼š{markets_str}")
        history.append((user_input, f"ğŸ“‹ ç­–ç•¥æ˜ç»†ï¼š\n{explanation}\n\nğŸ” æ­£åœ¨æ‰«æ {markets_str} (æœ€å¤šåˆ†æ {max_stocks} æ”¯)..."))

        # è·å–è‚¡ç¥¨åˆ—è¡¨
        stock_list = StockAnalyzer.get_stock_list(
            tuple(market_type),
            max_count=max_stocks,
            strategy_mode=strategy_mode
        )
        if not stock_list:
            history.append(("", "âš ï¸ è·å–è‚¡ç¥¨åˆ—è¡¨å¤±è´¥"))
            return history, history

        scored_stocks = analyze_stocks(stock_list, strategy_items, custom_weights, max_stocks, strategy_mode)

        if not scored_stocks:
            history.append(("", "âŒ æ²¡æœ‰æ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„è‚¡ç¥¨"))
        else:
            result_msg = "âœ… æ¨èè‚¡ç¥¨ (æŒ‰åˆ†æ•°æ’åº):\n"
            
            # æ·»åŠ é’ˆå¯¹ç©¿çº¿å‹ç­–ç•¥çš„ç‰¹æ®Šæç¤º
            if strategy_mode == "ç©¿çº¿å‹" and len(scored_stocks) > 10:
                result_msg += f"ğŸ” æ‰¾åˆ° {len(scored_stocks)} æ”¯æ»¡è¶³ä¸€é˜³ç©¿ä¸‰çº¿æ¡ä»¶çš„è‚¡ç¥¨\n"
                
            result_msg += "æ’å | ä»£ç  | åç§° | å¾—åˆ† | 5æ—¥æ¶¨å¹… | ä»“ä½ | åŒ¹é…ç­–ç•¥ | é£é™©æç¤º\n"
            result_msg += "-" * 100 + "\n"
            
            # å¦‚æœæ˜¯ç©¿çº¿å‹ç­–ç•¥ä¸”ç»“æœè¿‡å¤šï¼Œè€ƒè™‘åˆ†é¡µæ˜¾ç¤ºæˆ–é™åˆ¶ç»“æœè¡Œæ•°é¿å…UIæ˜¾ç¤ºé—®é¢˜
            max_display = len(scored_stocks)
            if strategy_mode == "ç©¿çº¿å‹" and max_display > 50:
                max_display = 50  # UIæ˜¾ç¤ºé™åˆ¶ï¼Œæœ€å¤šæ˜¾ç¤º50è¡Œ
            
            # ========== å…³é”®ä¿®æ”¹å¼€å§‹ï¼šUIæ˜¾ç¤ºè¿‡æ»¤ ==========
            valid_strategies = list(STRATEGY_WEIGHTS.keys()) + POTENTIAL_SIGNALS  # æœ‰æ•ˆç­–ç•¥åˆ—è¡¨
            for i, (score, ts_code, name, matched, pct_change, _, score_details) in enumerate(scored_stocks[:max_display], 1):
                # æ¸…æ´—ç­–ç•¥åˆ—è¡¨
                clean_matched = [s for s in matched if s in valid_strategies]

                # å¦‚æœåŒ…å«â€œä¸€é˜³ç©¿ä¸‰çº¿â€ï¼Œåœ¨å…¶åæ·»åŠ è¯„åˆ†æ ‡ç­¾
                if "ä¸€é˜³ç©¿ä¸‰çº¿" in clean_matched and isinstance(score_details, dict):
                    quality = score_details.get("ç©¿çº¿è¯„åˆ†", "")
                    if quality:
                        index = clean_matched.index("ä¸€é˜³ç©¿ä¸‰çº¿")
                        clean_matched[index] = f"ä¸€é˜³ç©¿ä¸‰çº¿ï¼ˆ{quality}ï¼‰"

                # æå–ä¹°ç‚¹æç¤º
                buy_signals = [s for s in clean_matched if s in KEY_BUY_SIGNALS]
                buy_info = "ğŸ”¥ä¹°ç‚¹:" + ",".join(buy_signals) if buy_signals else ""

                # æå–é£é™©æç¤º
                risk_warnings = []
                if isinstance(score_details, dict) and 'risk_warnings' in score_details:
                    risk_warnings = score_details['risk_warnings']
                risk_info = " | ".join(risk_warnings) if risk_warnings else "æ— "

                position = calculate_position(score, pct_change, risk_warnings, strategy_mode)

                # è¾“å‡ºæœ€ç»ˆç»“æœè¡Œ
                result_msg += (
                    f"{i:2d}. {ts_code.split('.')[0]} {name[:10]} | "
                    f"ğŸ“Š{int(score)} | "
                    f"ğŸ“ˆ{pct_change:.1f}% | "
                    f"âš–ï¸{position} | "
                    f"{buy_info} {', '.join(clean_matched[:3])} | "
                    f"{risk_info}\n"
                )

            # ========== å…³é”®ä¿®æ”¹ç»“æŸ ==========
            
            # å¦‚æœæœ‰æ›´å¤šç»“æœæœªæ˜¾ç¤ºï¼Œæ·»åŠ æç¤º
            if len(scored_stocks) > max_display:
                result_msg += f"\n... è¿˜æœ‰ {len(scored_stocks) - max_display} æ”¯æ»¡è¶³æ¡ä»¶çš„è‚¡ç¥¨æœªæ˜¾ç¤º (æ€»å…± {len(scored_stocks)} æ”¯) ..."
                
            history.append(("", result_msg))

    except Exception as e:
        logger.error(f"ç•Œé¢äº¤äº’é”™è¯¯: {str(e)}", exc_info=True)
        history.append(("", f"âš ï¸ ç³»ç»Ÿé”™è¯¯: {str(e)}"))

    return history, history



def query_stock(ts_code: str) -> str:
    stock_info = StockAnalyzer.get_single_stock_info(ts_code)
    if not stock_info:
        return "âŒ æœªæ‰¾åˆ°è¯¥è‚¡ç¥¨æˆ–æ•°æ®è·å–å¤±è´¥"
    
    basic = stock_info['basic_info']
    price = stock_info['price_info']
    signals = stock_info['technical_signals']
    signal_msgs = [f"ğŸ”¹ {s}: {'âœ…' if v else 'âŒ'}" for s, v in signals.items() if v]
    
    return f"""
ğŸ“ˆ è‚¡ç¥¨ä¿¡æ¯ [{basic['ts_code']}]
----------------------------
åç§°ï¼š{basic['name']}
è¡Œä¸šï¼š{basic.get('industry', 'N/A')}
ä¸Šå¸‚æ—¥æœŸï¼š{basic['list_date']}
å¸‚åœºï¼š{basic['market']}

ğŸ’µ æœ€æ–°è¡Œæƒ…ï¼ˆ{price['trade_date']}ï¼‰
----------------------------
æ”¶ç›˜ä»·ï¼š{price['close']}
æ¶¨è·Œå¹…ï¼š{price['pct_chg']}%
æˆäº¤é‡ï¼š{price['vol']/10000:.2f}ä¸‡æ‰‹
æˆäº¤é¢ï¼š{price['amount']/10000:.2f}ä¸‡å…ƒ

ğŸ“Š æŠ€æœ¯ä¿¡å·
----------------------------
{'\n'.join(signal_msgs) if signal_msgs else 'âš ï¸ æœªè§¦å‘ä»»ä½•æŠ€æœ¯ä¿¡å·'}
"""

# ===== é»˜è®¤å€¼é…ç½® =====
DEFAULT_TURNOVER = 8000   # ä»Šæ—¥æˆäº¤é¢é»˜è®¤å€¼ï¼ˆäº¿å…ƒï¼‰
DEFAULT_AVG_TURNOVER = 9000  # è¿‘30æ—¥å¹³å‡æˆäº¤é¢é»˜è®¤å€¼ï¼ˆäº¿å…ƒï¼‰
# ===== æ¶¨åœæ•°æ®æ–‡ä»¶ç¼“å­˜ =====
LIMIT_UP_CACHE_FILE = 'limit_up_cache.json'

# å¯åŠ¨æ—¶åŠ è½½ç¼“å­˜
if os.path.exists(LIMIT_UP_CACHE_FILE):
    with open(LIMIT_UP_CACHE_FILE, 'r', encoding='utf-8') as f:
        _limit_up_cache = json.load(f)
else:
    _limit_up_cache = {}

# ===== ç¼“å­˜ä»Šæ—¥è¡Œæƒ…æ•°æ®ï¼Œé¿å…é‡å¤è°ƒç”¨ =====
_daily_data_cache = None
def get_last_trade_date() -> str:
    """è·å–æœ€è¿‘ä¸€ä¸ªäº¤æ˜“æ—¥"""
    today = datetime.today()
    while today.weekday() >= 5:  # å‘¨å…­æ—¥
        today -= timedelta(days=1)
    return today.strftime('%Y%m%d')

def get_daily_data(trade_date: str):
    global _daily_data_cache
    if _daily_data_cache is None or _daily_data_cache.get('date') != trade_date:
        df = safe_api_call(pro.daily, trade_date=trade_date, fields='ts_code,pct_chg')
        _daily_data_cache = {'date': trade_date, 'data': df}
    return _daily_data_cache['data'] if _daily_data_cache else pd.DataFrame()

def get_up_stock_count(trade_date: str) -> int:
    try:
        df = get_daily_data(trade_date)
        return (df['pct_chg'] > 0).sum() if not df.empty else 0
    except:
        return 0

def get_down_stock_count(trade_date: str) -> int:
    try:
        df = get_daily_data(trade_date)
        return (df['pct_chg'] < 0).sum() if not df.empty else 0
    except:
        return 0

def get_today_turnover(trade_date: str) -> float:
    """è·å–ä¸Šè¯æŒ‡æ•°çš„æˆäº¤é¢ï¼ˆäº¿å…ƒï¼‰"""
    try:
        df = safe_api_call(pro.index_daily, ts_code='000001.SH', trade_date=trade_date, fields='trade_date,vol,amount')
        if not df.empty and 'amount' in df.columns:
            return df['amount'].iloc[0] / 10000  # æ¢ç®—ä¸ºäº¿å…ƒ
        return DEFAULT_TURNOVER
    except:
        return DEFAULT_TURNOVER

def get_avg_turnover_30d(trade_date: str) -> float:
    """è·å–ä¸Šè¯æŒ‡æ•°è¿‘30æ—¥å¹³å‡æˆäº¤é¢ï¼ˆäº¿å…ƒï¼‰"""
    try:
        end = datetime.strptime(trade_date, '%Y%m%d')
        start = end - timedelta(days=40)
        df = safe_api_call(pro.index_daily, ts_code='000001.SH', start_date=start.strftime('%Y%m%d'), end_date=end.strftime('%Y%m%d'), fields='trade_date,amount')
        if not df.empty:
            return df['amount'].mean() / 10000
        return DEFAULT_AVG_TURNOVER
    except:
        return DEFAULT_AVG_TURNOVER


def get_limit_up_count(trade_date: str) -> int:
    """è·å–å½“æ—¥æ¶¨åœè‚¡æ•°é‡ï¼Œå¸¦æ–‡ä»¶ç¼“å­˜"""
    if trade_date in _limit_up_cache:
        logger.info(f"[æ¶¨åœç»Ÿè®¡] ä½¿ç”¨ç¼“å­˜æ•°æ®ï¼š{trade_date} æ¶¨åœè‚¡æ•°é‡ï¼š{_limit_up_cache[trade_date]}")
        return _limit_up_cache[trade_date]

    try:
        df = safe_api_call(
            pro.limit_list_d,
            trade_date=trade_date,
            limit_type='U',
            fields='ts_code'
        )
        if df is None or df.empty:
            logger.warning(f"[æ¶¨åœç»Ÿè®¡] {trade_date} æ— æ¶¨åœæ•°æ®è¿”å›ï¼Œç¼“å­˜ä¸º0")
            _limit_up_cache[trade_date] = 0
            save_limit_up_cache()
            return 0

        count = len(df)
        _limit_up_cache[trade_date] = count
        save_limit_up_cache()
        logger.info(f"[æ¶¨åœç»Ÿè®¡] {trade_date} æ¶¨åœè‚¡æ•°é‡ï¼š{count}")
        return count

    except Exception as e:
        logger.error(f"[æ¶¨åœç»Ÿè®¡] è·å–æ¶¨åœæ•°æ®å¤±è´¥ï¼š{e}ï¼Œç¼“å­˜ä¸º0")
        _limit_up_cache[trade_date] = 0
        save_limit_up_cache()
        return 0


def save_limit_up_cache():
    with open(LIMIT_UP_CACHE_FILE, 'w', encoding='utf-8') as f:
        json.dump(_limit_up_cache, f, ensure_ascii=False, indent=2)

# ===== æ–°å¢ï¼šå¼ºåˆ¶åˆ·æ–°å¸‚åœºæƒ…ç»ª =====
def refresh_market_sentiment() -> str:
    global _last_sentiment_date, _cached_sentiment, _limit_up_cache
    # 1) æ¸…æ‰ä»Šæ—¥æƒ…ç»ªç¼“å­˜
    _last_sentiment_date = None
    _cached_sentiment = None

    # 2) æ¸…æ‰å½“æ—¥æ¶¨åœè‚¡æ–‡ä»¶ç¼“å­˜ï¼Œè®© get_limit_up_count é‡æ–°æ‹‰å–
    today = datetime.today().strftime('%Y%m%d')
    trade_date = get_last_trade_date() if datetime.today().weekday() >= 5 else today
    if trade_date in _limit_up_cache:
        del _limit_up_cache[trade_date]
        save_limit_up_cache()

    # 3) å†æ¬¡è¿”å›æœ€æ–°çš„å¸‚åœºæƒ…ç»ª
    return get_market_sentiment_ui()

# ===== å¸‚åœºæƒ…ç»ªç¼“å­˜æœºåˆ¶ =====
_last_sentiment_date = None
_cached_sentiment = None

def get_market_sentiment_ui():
    global _last_sentiment_date, _cached_sentiment
    today = datetime.today().strftime('%Y%m%d')

    # åˆ¤æ–­æ˜¯å¦ä¼‘å¸‚
    is_weekend = datetime.today().weekday() >= 5
    trade_date = get_last_trade_date() if is_weekend else today

    if _last_sentiment_date == today and _cached_sentiment:
        sentiment, description = _cached_sentiment
    else:
        sentiment, description = calculate_market_sentiment()
        _cached_sentiment = (sentiment, description)
        _last_sentiment_date = today

    # æ ¼å¼åŒ–æ—¥æœŸæ˜¾ç¤º
    display_date = datetime.strptime(trade_date, '%Y%m%d').strftime('%Y-%m-%d')
    notice = f"ğŸ“… æ•°æ®æ—¥æœŸï¼š{display_date}"
    if is_weekend:
        notice += " ï¼ˆä»Šæ—¥ä¼‘å¸‚ï¼Œå±•ç¤ºæœ€è¿‘äº¤æ˜“æ—¥æ•°æ®ï¼‰"

    return f"{notice}\n\nğŸ“Š å½“å‰å¸‚åœºæƒ…ç»ªï¼š**{sentiment}**\n\n{description}"


def calculate_market_sentiment() -> Tuple[str, str]:
    # åˆ¤æ–­æ˜¯å¦ä¼‘å¸‚
    is_weekend = datetime.today().weekday() >= 5
    trade_date = get_last_trade_date() if is_weekend else datetime.today().strftime('%Y%m%d')

    if is_weekend:
        logger.info(f"[å¸‚åœºæƒ…ç»ª] ä»Šæ—¥ä¼‘å¸‚ï¼Œä½¿ç”¨æœ€è¿‘äº¤æ˜“æ—¥æ•°æ®ï¼š{trade_date}")
    else:
        logger.info(f"[å¸‚åœºæƒ…ç»ª] ä½¿ç”¨ä»Šæ—¥æ•°æ®ï¼š{trade_date}")

    # è·å–æ•°æ®ï¼ˆè¿™é‡Œä½ éœ€è¦æ”¹é€ å„ä¸ªæ–¹æ³•æ”¯æŒä¼ å…¥ trade_dateï¼‰
    up_count = get_up_stock_count(trade_date)
    down_count = get_down_stock_count(trade_date)
    turnover = get_today_turnover(trade_date)
    avg_turnover = get_avg_turnover_30d(trade_date)
    limit_up_count = get_limit_up_count(trade_date)

    logger.info(f"[å¸‚åœºæƒ…ç»ª] æŒ‡æ ‡ => ä¸Šæ¶¨: {up_count} | ä¸‹è·Œ: {down_count} | æˆäº¤é¢: {turnover:.2f} äº¿ | 30æ—¥å‡é‡: {avg_turnover:.2f} äº¿ | æ¶¨åœæ•°: {limit_up_count}")

    score = 0
    up_down_score = (up_count / (down_count + 1)) * 20
    turnover_score = (turnover / avg_turnover) * 30
    limit_up_score = min(limit_up_count, 50) * 1.5

    score = up_down_score + turnover_score + limit_up_score

    logger.info(f"[å¸‚åœºæƒ…ç»ª] è¯„åˆ† => æ¶¨è·Œæ¯”: {up_down_score:.2f} | æˆäº¤é¢: {turnover_score:.2f} | æ¶¨åœ: {limit_up_score:.2f} | æ€»åˆ†: {score:.2f}")

    if score > 100:
        sentiment = "ä¹è§‚"
        description = "ğŸ˜„ å½“å‰å¸‚åœºæ´»è·ƒï¼Œé¢˜æè½®åŠ¨åŠ å¿«ï¼Œé€‚åˆçŸ­çº¿æ“ä½œ"
    elif score < 60:
        sentiment = "æ‚²è§‚"
        description = "ğŸ˜Ÿ å¸‚åœºä½è¿·ï¼Œæ³¨æ„æ§åˆ¶é£é™©ï¼Œé˜²å®ˆä¸ºä¸»"
    else:
        sentiment = "éœ‡è¡"
        description = "ğŸ˜ å¸‚åœºè§‚æœ›æƒ…ç»ªæµ“åšï¼Œç²¾é€‰ä¼˜è´¨æ ‡çš„"

    logger.info(f"[å¸‚åœºæƒ…ç»ª] æœ€ç»ˆåˆ¤æ–­ï¼š{sentiment}ï¼ˆ{score:.2f} åˆ†ï¼‰")

    return sentiment, description

# ===== åˆ›å»ºGradioç•Œé¢ =====
with gr.Blocks(theme=gr.themes.Soft()) as demo:
    gr.Markdown("""
    # ğŸ“ˆ AIé‡åŒ–é€‰è‚¡ç³»ç»Ÿ (V25.5.15)
    **åŠŸèƒ½**:
    - ä½¿ç”¨Tushareè·å–å½“æ—¥æ•°æ®ï¼ˆæ™šä¸Š8ç‚¹å·¦å³æ›´æ–°å®Œæ¯•ï¼‰            
    - æ”¯æŒè‡ªç„¶è¯­è¨€ç­–ç•¥è¾“å…¥
    - æŠ€æœ¯æŒ‡æ ‡åˆ†æ
    - æ¨èå†å²è®°å½•ç®¡ç†
    - ä¸ªè‚¡è¯¦æƒ…æŸ¥è¯¢
    """)

    # ===== å¸‚åœºæƒ…ç»ªå±•ç¤º =====
    with gr.Row():
        sentiment_box = gr.Markdown(get_market_sentiment_ui())
        refresh_sentiment_btn = gr.Button("ğŸ”„ åˆ·æ–°å¸‚åœºæƒ…ç»ª")
        refresh_sentiment_btn.click(fn=refresh_market_sentiment, outputs=sentiment_box)

    with gr.Row():
        with gr.Column(scale=3):
            # é€‰è‚¡åˆ†æç»“æœçª—å£ï¼ˆä¿ç•™ï¼‰
            chatbot = gr.Chatbot(height=500, label="é€‰è‚¡åˆ†æ")
            
            # è¾“å…¥æ¡†ï¼ˆç§»é™¤æˆ–éšè—ï¼‰
            # txt = gr.Textbox(label="è¾“å…¥é€‰è‚¡ç­–ç•¥", 
            #                  placeholder="ä¾‹å¦‚: æ‰¾å‡ºMACDé‡‘å‰ä¸”æˆäº¤é‡æ”¾å¤§çš„è‚¡ç¥¨ï¼Œå¦‚æœä½ æƒ³ç›´æ¥è·å–æ™ºèƒ½æ¨èï¼Œå¯ä»¥è¾“å…¥"æ¨èè‚¡ç¥¨"",
            #                  visible=False)  # æˆ–è€…ç›´æ¥æ³¨é‡Šæ‰
            
        with gr.Column(scale=1):
            market_type = gr.CheckboxGroup(
                choices=list(MARKET_SECTORS.keys()),
                value=["ä¸»æ¿"],
                label="é€‰æ‹©å¸‚åœº/æ¿å—"
            )
            with gr.Accordion("é«˜çº§é€‰é¡¹", open=True):
                max_stocks = gr.Slider(
                    minimum=100,
                    maximum=5500,
                    step=100,
                    value=Config.MAX_STOCKS_TO_ANALYZE,
                    label="æœ€å¤§åˆ†æè‚¡ç¥¨æ•°é‡"
                )
                strategy_mode = gr.Radio(
                    choices=["ç¨³å¥å‹", "æ¿€è¿›å‹", "ç©¿çº¿å‹"], 
                    value="ç¨³å¥å‹",
                    label="ç­–ç•¥æ¨¡å¼é€‰æ‹©"
                )
            clear_btn = gr.Button("æ¸…é™¤è®°å½•", variant="secondary")
            analyze_btn = gr.Button("å¼€å§‹åˆ†æ", variant="primary")

    # ===== æ¨èå†å² Tab =====
    with gr.Tab("ğŸ“Š æ¨èå†å²"):
        tracking_html = gr.HTML(value=get_tracking_html())
        refresh_btn = gr.Button("ğŸ”„ åˆ·æ–°æ¨èå†å²", variant="primary")
        refresh_btn.click(fn=get_tracking_html, outputs=tracking_html)

        with gr.Row():
            delete_code = gr.Textbox(label="è¾“å…¥è¦åˆ é™¤çš„è‚¡ç¥¨ä»£ç ", placeholder="ä¾‹å¦‚: 000001 æˆ– 000001.SZ", scale=4)
            delete_btn = gr.Button("ğŸ—‘ï¸ åˆ é™¤", variant="stop", scale=1)

        with gr.Row():
            clear_all_btn = gr.Button("ğŸ’£ æ¸…ç©ºæ‰€æœ‰è®°å½•", variant="stop")

        confirm_clear = gr.Checkbox(label="ç¡®è®¤æ¸…ç©ºæ‰€æœ‰è®°å½•", visible=False)
        status_msg = gr.Textbox(visible=False)
        delete_msg = gr.Textbox(visible=False)

        delete_btn.click(
            fn=lambda code: tracker.remove_stock(code),
            inputs=delete_code,
            outputs=delete_msg
        ).then(fn=lambda: "", outputs=delete_code).then(fn=get_tracking_html, outputs=tracking_html)

        def toggle_confirm_clear():
            return {"visible": True}, "è¯·å‹¾é€‰ç¡®è®¤æ¡†åå†æ¬¡ç‚¹å‡»æ¸…ç©ºæŒ‰é’®"

        clear_all_btn.click(fn=toggle_confirm_clear, outputs=[confirm_clear, status_msg])

        def handle_clear_confirmation(confirmed):
            if confirmed:
                tracker.clear_all_recommendations(confirmation=True)
                return {"visible": False}, "âœ… å·²æ¸…ç©ºæ‰€æœ‰è®°å½•", get_tracking_html()
            else:
                return {"visible": False}, "âŒ æ¸…ç©ºæ“ä½œå·²å–æ¶ˆ", get_tracking_html()

        confirm_clear.change(
            fn=handle_clear_confirmation,
            inputs=confirm_clear,
            outputs=[confirm_clear, status_msg, tracking_html]
        )

    # ===== ä¸ªè‚¡æŸ¥è¯¢ Tab =====
    with gr.Tab("ğŸ” ä¸ªè‚¡æŸ¥è¯¢"):
        stock_query = gr.Textbox(label="è¾“å…¥è‚¡ç¥¨ä»£ç ï¼ˆå¦‚ï¼š000006æˆ–000006.SZï¼‰")
        query_btn = gr.Button("æŸ¥è¯¢")
        stock_output = gr.Textbox(label="æŸ¥è¯¢ç»“æœ", interactive=False)
        query_btn.click(fn=query_stock, inputs=stock_query, outputs=stock_output)

    history = gr.State([])

    # ä¿®æ”¹åˆ†ææŒ‰é’®çš„å¤„ç†å‡½æ•°ï¼Œç›´æ¥è°ƒç”¨æ¨èè‚¡ç¥¨
    def analyze_without_input(market_type, max_stocks, strategy_mode, history):
        return chat_interface("æ¨èè‚¡ç¥¨", market_type, max_stocks, strategy_mode, history)

    # ç»‘å®šäº‹ä»¶å¤„ç†
    analyze_btn.click(
        analyze_without_input, 
        [market_type, max_stocks, strategy_mode, history], 
        [chatbot, history]
    )
    
    clear_btn.click(lambda: ([], []), None, [chatbot, history])



# ===== å¯åŠ¨æœåŠ¡ =====
if __name__ == "__main__":
    demo.launch(server_port=7860, share=False)
    
